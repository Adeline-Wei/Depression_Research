{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import re, math\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from random import randint\n",
    "# import ipynb.fs.defs.PeopleInfo as peopleInfo\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import TweetTokenizer\n",
    "from nltk import PorterStemmer\n",
    "import ipynb.fs.defs.FilterMethods as filterMethods\n",
    "import sys, os\n",
    "sys.path.append('../2_feature')\n",
    "import ipynb.fs.defs.GetFeatures as getFeatures\n",
    "global stop_words\n",
    "stop_words = stopwords.words('english')\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from IPython.display import display, Image\n",
    "plotly.tools.set_credentials_file(username='Adeline', api_key='Z5eltNtBQXqvI05ZFQtz')\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "import datetime\n",
    "from matplotlib import dates\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "import matplotlib.dates as mdates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patient(object):\n",
    "    totalCount = 0\n",
    "    diagnosedTimeDict = dict()\n",
    "    idToNameDict = dict()\n",
    "    with open('../0_dataset/diagnosedTweetsPatch2') as open_file:\n",
    "        for line in open_file.readlines():\n",
    "            line = line.strip().split('\\t')\n",
    "            diagnosedTimeDict[line[0]] = datetime.datetime.strptime(line[3], \"%Y-%m-%d %H:%M:%S\")\n",
    "            idToNameDict[line[0]] = line[1]\n",
    "       \n",
    "    def __init__(self, userId):\n",
    "        self.userId = userId\n",
    "        self.name = Patient.idToNameDict[userId]\n",
    "        self.df0 = pd.read_csv('../0_dataset/DepressionUsersTweets/Patch2_data/'+userId,sep='\\t',header=None,usecols=[2,3],names=['Date','Text'],quoting=3,error_bad_lines=False,encoding='utf-8').dropna(axis=0, how='any')\n",
    "        self.df0 = self.df0.set_index('Date')\n",
    "        self.df0.index = pd.to_datetime(self.df0.index)\n",
    "\n",
    "        self.diagnosedTime = Patient.diagnosedTimeDict[userId]\n",
    "        timeDuration1 = self.diagnosedTime - datetime.timedelta(days=90)\n",
    "        timeDuration2 = self.diagnosedTime - datetime.timedelta(days=180)\n",
    "        self.df1 = self.df0.loc[str(self.diagnosedTime):str(timeDuration1)]\n",
    "        self.df2 = self.df0.loc[str(timeDuration1):str(timeDuration2)]\n",
    "        Patient.totalCount += 1 \n",
    "        \n",
    "    def displayCount(self):\n",
    "        print(\"Total Patients {0}\".format(Patient.totalCount))\n",
    "    \n",
    "    def displayTweetsCount(self, category=1):\n",
    "        if category == 0:\n",
    "            return len(self.df0)\n",
    "        elif category == 1:\n",
    "            return len(self.df1)\n",
    "        elif category == 2:\n",
    "            return len(self.df2)\n",
    "        else:\n",
    "            print('Please indicate category (inRange or all)')\n",
    "            return False\n",
    "        \n",
    "    def getText(self, category=1):\n",
    "        if category == 0:\n",
    "            return self.df0['Text'].values\n",
    "        if category == 1:\n",
    "            return self.df1['Text'].values\n",
    "        elif category == 2:\n",
    "            return self.df2['Text'].values\n",
    "        else:\n",
    "            print('Please indicate category (inRange or all)')\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_by_symbols(txt):\n",
    "    txt = re.sub(r\"https\\S+\", '', txt)\n",
    "    txt = re.sub(r\"http\\S+\", '', txt)\n",
    "    txt = re.sub(r\"pic.twitter.com\\S+\", '', txt)\n",
    "    txt = re.sub(r\"twitter.com/\\S+\", '', txt)\n",
    "    txt = re.sub(r\"\\S+/\\S+\", '', txt)\n",
    "    txt = re.sub(r\"@\\S+\", '', txt)\n",
    "    txt = re.sub(r\"#\\S+\", '', txt)\n",
    "    txt = re.sub(r\"idk\", 'i do not know', txt)   # idk: i don't know\n",
    "    txt = re.sub(r\"tbh\", 'to be honest', txt)   # tbh: to be honest\n",
    "    txt = re.sub(r\"tho\", 'though', txt)   # tho\n",
    "    txt = re.sub(r\"i\\'m\", 'i am', txt)\n",
    "    txt = re.sub(r\"you\\'re\", 'you are', txt)\n",
    "    txt = re.sub(r\"he\\'s\", 'he is', txt)\n",
    "    txt = re.sub(r\"she\\'s\", 'she is', txt)\n",
    "    txt = re.sub(r\"it\\'s\", 'it is', txt)\n",
    "    txt = re.sub(r\"we\\'re\", 'we are', txt)\n",
    "    txt = re.sub(r\"they\\'re\", 'they are', txt)\n",
    "    txt = re.sub(r\"isn\\'t\", 'is not', txt)\n",
    "    txt = re.sub(r\"don\\'t\", 'do not', txt)\n",
    "    txt = re.sub(r\"doesn\\'t\", 'does not', txt)\n",
    "    txt = re.sub(r\"didn\\'t\", 'did not', txt)\n",
    "    txt = re.sub(r\"wasn\\'t\", 'was not', txt)\n",
    "    txt = re.sub(r\"weren\\'t\", 'were not', txt)\n",
    "    txt = re.sub(r\"haven\\'t\", 'have not', txt)\n",
    "    txt = re.sub(r\"can\\'t\", 'can not', txt)\n",
    "    txt = re.sub(r\"couldn\\'t\", 'could not', txt)\n",
    "    txt = re.sub(r\"wouldn\\'t\", 'would not', txt)\n",
    "    txt = re.sub(r\"shouldn\\'t\", 'should not', txt)\n",
    "    txt = re.sub(r\"&amp\", '', txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.43 s, sys: 68 ms, total: 1.5 s\n",
      "Wall time: 1.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "patients = dict()\n",
    "with open('../0_dataset/patch2PatientIDs') as openFile:\n",
    "    for patient in openFile.readlines():\n",
    "        patient = patient.strip()\n",
    "        patients[patient] = Patient(patient)\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# base_texts = []\n",
    "group_texts = []\n",
    "\n",
    "for key in patients.keys():\n",
    "    group_texts.append(replace_by_symbols('\\n'.join(patients[key].getText(1))))\n",
    "\n",
    "# for key in ordinarys.keys():\n",
    "#     base_texts.append(replace_by_symbols('\\n'.join(ordinarys[key].getText())))\n",
    "\n",
    "# corpus = base_texts + group_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorizer(ngrams=1):\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\", token_pattern='(?u)\\\\b[a-zA-Z]\\\\w{2,}\\\\b', ngram_range = (ngrams,ngrams), min_df=1)\n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_scores(vectorizer, tfidf_result):\n",
    "    # http://stackoverflow.com/questions/16078015/\n",
    "    scores = zip(vectorizer.get_feature_names(),\n",
    "                 np.asarray(tfidf_result.sum(axis=0)).ravel())\n",
    "    sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    words = []\n",
    "    scores = []\n",
    "    for item in sorted_scores:\n",
    "        words.append(item[0])\n",
    "        scores.append(item[1])\n",
    "    return words, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = create_vectorizer()\n",
    "tfidf_result = vectorizer.fit_transform(group_texts)\n",
    "words, scores = display_scores(vectorizer, tfidf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "just 15.5710522482\n",
      "like 14.9442976943\n",
      "love 11.5760331508\n",
      "know 8.14639317976\n",
      "good 6.87465270101\n",
      "want 6.70333379776\n",
      "people 6.50405708417\n",
      "really 6.33557510248\n",
      "shit 6.06214344154\n",
      "fuck 6.04191544798\n",
      "time 5.74418016352\n",
      "got 5.59225987869\n",
      "today 5.5766771962\n",
      "day 5.55936927208\n",
      "did 5.41035565804\n",
      "fucking 5.31691932914\n",
      "going 5.16847757787\n",
      "think 5.06596968172\n",
      "need 4.92763499858\n",
      "new 4.69978960361\n",
      "gonna 4.65245657381\n",
      "feel 4.50048453158\n",
      "lol 4.45591093829\n",
      "life 4.44754772597\n",
      "happy 4.38990850656\n",
      "make 3.94688577686\n",
      "lmao 3.90652723362\n",
      "thank 3.77173831567\n",
      "god 3.72633669\n",
      "video 3.68074494519\n",
      "does 3.64053041784\n",
      "wanna 3.50040100928\n",
      "follow 3.49097951501\n",
      "right 3.45836030879\n",
      "hate 3.43533657602\n",
      "better 3.20420883639\n",
      "man 3.05512789452\n",
      "best 3.01779739779\n",
      "okay 3.00758546574\n",
      "look 3.00602663577\n",
      "thing 2.99519795718\n",
      "yes 2.98078145333\n",
      "omg 2.97419351604\n",
      "friends 2.9119888107\n",
      "let 2.90469612947\n",
      "bad 2.90251133436\n",
      "hope 2.84351825722\n",
      "work 2.82317582552\n",
      "actually 2.8015453372\n",
      "say 2.79317040021\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    print(words[i],scores[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter out users (Case1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch2FilterOutCase1 = []\n",
    "with open('patch2FilterOutCase1') as open_file:\n",
    "    for line in open_file.readlines():\n",
    "        line = line.strip()\n",
    "        patch2FilterOutCase1.append(line)\n",
    "        try:\n",
    "            del patients[line]\n",
    "        except KeyError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "group_texts = []\n",
    "\n",
    "for key in patients.keys():\n",
    "    group_texts.append(replace_by_symbols('\\n'.join(patients[key].getText(1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = create_vectorizer()\n",
    "tfidf_result = vectorizer.fit_transform(group_texts)\n",
    "words, scores = display_scores(vectorizer, tfidf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "just 12.5693990997\n",
      "like 12.4306876504\n",
      "love 9.07527699758\n",
      "know 6.6528181685\n",
      "good 5.7216059736\n",
      "really 5.3027535116\n",
      "want 5.27326990107\n",
      "people 5.19184026681\n",
      "fuck 5.0079775563\n",
      "shit 4.99594358312\n",
      "time 4.79412937591\n",
      "day 4.63950289237\n",
      "got 4.53774766959\n",
      "did 4.35256194845\n",
      "fucking 4.32115320006\n",
      "today 4.27290431912\n",
      "think 4.12787881051\n",
      "going 4.04178678479\n",
      "gonna 3.88146080441\n",
      "need 3.83240002137\n",
      "new 3.78167009083\n",
      "lol 3.66843185208\n",
      "happy 3.58478404255\n",
      "feel 3.53966287542\n",
      "life 3.51191572816\n",
      "lmao 3.41314318803\n",
      "god 3.31839401953\n",
      "make 3.17128102804\n",
      "thank 2.97127675015\n",
      "wanna 2.90731860242\n",
      "right 2.73336646218\n",
      "hate 2.72662150802\n",
      "video 2.60991466044\n",
      "man 2.55823758137\n",
      "does 2.55172257944\n",
      "work 2.49138633262\n",
      "let 2.49134838183\n",
      "thing 2.43784951112\n",
      "better 2.43610531823\n",
      "look 2.43285245348\n",
      "best 2.41861538929\n",
      "omg 2.35168481564\n",
      "yes 2.33455000143\n",
      "yeah 2.3145499956\n",
      "actually 2.29906762588\n",
      "okay 2.29126109399\n",
      "say 2.28808025758\n",
      "hope 2.26109163709\n",
      "bad 2.23536299484\n",
      "night 2.20973465369\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    print(words[i],scores[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter (Case2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomSelectRows(userId, rowsPerDay=5):\n",
    "    \n",
    "    indices = []\n",
    "    values = []\n",
    "    df = patients[userId].df1\n",
    "    if patients[userId].displayTweetsCount(1) <= 450:\n",
    "        indices.extend(df.index)\n",
    "    else:\n",
    "        gp = df.index.groupby(df.index.day)\n",
    "\n",
    "        for key in gp.keys():\n",
    "            rowIndex = np.random.choice(gp[key], rowsPerDay)\n",
    "            indices.extend(pd.to_datetime(rowIndex))\n",
    "        indices = list(set(indices))\n",
    "        i = len(indices)\n",
    "        while i < 450:\n",
    "            tmpIndices = np.random.choice(df, 1)\n",
    "            print('{0}\\t{1}'.format(i, tmpIndices))\n",
    "            if tmpIndices in indices:\n",
    "                continue\n",
    "            else:\n",
    "                indices.extend(tmpIndices)\n",
    "                i += 1\n",
    "    for index in indices:\n",
    "        values.append(df.loc[index]['Text'])\n",
    "    \n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "a must be 1-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-157-c24312e9a727>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#     texts = randomSelectRows(patient)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#     break\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandomSelectRows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'3010786498'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-152-67946aac4130>\u001b[0m in \u001b[0;36mrandomSelectRows\u001b[0;34m(userId, rowsPerDay)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m450\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mtmpIndices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{0}\\t{1}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmpIndices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtmpIndices\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice (numpy/random/mtrand/mtrand.c:17151)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: a must be 1-dimensional"
     ]
    }
   ],
   "source": [
    "# 每人每天5篇，總共450篇（不滿就全取）\n",
    "# for patient in patients:\n",
    "#     texts = randomSelectRows(patient)\n",
    "#     break\n",
    "texts = randomSelectRows('3010786498')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"http://www. beyondblue.org.au I was diagnosed with depression today - terrified. I'm getting help, so can you. See your GP. It gets better.\",\n",
       " \"http://www. beyondblue.org.au #mentalhealth There's no shame in talking about it... See your GP today!\",\n",
       " 'Breakups are difficult. How do you know what your true feelings are when both parties involved have depression?',\n",
       " \"@MatthewSantoro Its always about the sensationalism. Ratings and all that jazz... sad isn't it?\",\n",
       " \"@derekhough You'd be surprised to know Australia has an ugly side, much like America's only less classy. U.S. has guns, Aus has xenophobia\",\n",
       " 'Australia Day, the day our country celebrates being a bunch of racist xenopbobes. Not having Lamb tonight, ordered pizza instead. #shameful',\n",
       " '@NicoleArbour \"98% of male YouTubers have never seen a vagina in person\", but I bet you\\'ve seen more than your fair share, lesbian fascist.',\n",
       " \"@NicoleArbour Sad you couldn't make it to the White House, or pissed you missed another opportunity to be an ass?\",\n",
       " 'Nicole Arbour EXPOSED As A Liar - With Proof (Vs Matthew Santoro)... Care to respond, @NicoleArbour ? http:// youtu.be/B5LlpqU3YpI',\n",
       " 'Watched @NicoleArbour response video... \"All about me, all about me\". Newsflash Nicole, it ain\\'t! You\\'re the sad one.',\n",
       " 'Shame on @NicoleArbour - trying to cover up your actions is #shamefull girl... Do unto others sweetheart!',\n",
       " 'Huge fan of @MatthewSantoro - great to see a man talking about this important issue. Men are often victims too. #DomesticViolence',\n",
       " \"I asked who'd be next... My sympathies to Celine Dion on the passing of her husband, Rene Angelil...\",\n",
       " 'Celebrities dropping like flies lately... Natalie Cole, David Bowie, Alan Rickman... who next, I fear?',\n",
       " 'Missing my niece Bree in Sydney...',\n",
       " '#NapoleonSyndrome Poor @realDonaldTrump the little man that he is... the only thing virile about him is his potty mouth!',\n",
       " \"@piersmorgan #BBL05 If thats how you treat female reporters in the UK, then we Aussies don't want it Down Under. #shameful\",\n",
       " 'My birthday in Saturday. Never did like it so soon after Christmas. #CapricornsRule',\n",
       " '@derekhough Experiencing similar. #newyearblues',\n",
       " '@andywestTV Its a total shame that homophobia is still being idolised in the media. #istandwithandywest',\n",
       " 'So totally missing @BindiIrwin and @derekhough on dwts...',\n",
       " '#dotherightthing https://www. causes.com/actions/177119 6-pledge-to-speak-up-for-all-animals?conversion_request_id=327753814&recruiter_id=13781583&utm_campaign=share&utm_medium=update&utm_source=tw …',\n",
       " '@stephenfry Come, Stephen, be adventurous! Life is not being afraid to try new things.',\n",
       " \"If @Channel7 had a brain, they'd cut half of The Big Bang Theory to make room for Grey's Anatomy. Who cares about terrorist updates? #shame\",\n",
       " 'Image from http:// media.nbcmiami.com/images/1200*67 5/france+flag.jpg … . #jesuisparisenne #JeSuisFrancais #IStandWithParis pic.twitter.com/X7DRjjftOK',\n",
       " '#istandwithparis',\n",
       " '@7NewsQueensland The death penalty has no place in Australia. Makes system equal to murderers. #shameful',\n",
       " \"@billshortenmp Centrelink is targeting under-35's on DSP in a bid to save money... #shameful\",\n",
       " 'Sign the petition: Sign the Petition to Save Our Weekend http://www. saveourweekend.org.au/weekend_rates? recruiter_id=20778 …',\n",
       " '#lestweforget pic.twitter.com/DeEm0vs3YS',\n",
       " \"#xfactorau Bored with the crap music & infighting between the judges, turned me right off the show so much that I didn't even watch tonight!\",\n",
       " '#LGBT #marriagequality @TurnbullMalcolm Tick tick, Malcom... pull your finger out and get with it!!',\n",
       " '@QLDLabor @AustralianLabor @rainbowlaborqld I joined the Labor Party to make a difference. Joining was easy - http://www. alp.org.au /']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
