{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import ipynb.fs.defs.PeopleInfo as peopleInfo\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import TweetTokenizer\n",
    "# from nltk.stem.porter import PorterStemmer\n",
    "# 引用 tweetTextHandler.ipynb\n",
    "import ipynb.fs.defs.TweetTextHandler as tweetTextHandler\n",
    "import ipynb.fs.defs.FilterMethods as filterMethods\n",
    "import sys, os\n",
    "sys.path.append('../2_feature')\n",
    "import ipynb.fs.defs.GetFeatures as getFeatures\n",
    "global stop_words\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getWordsDict(texts):\n",
    "    global stop_words\n",
    "    wordDict = dict()\n",
    "    stopwordDict = dict()\n",
    "    stopwordTweetCount = 0\n",
    "    tknzr = TweetTokenizer()\n",
    "\n",
    "    for txt in texts:\n",
    "        stopwordFlag = False\n",
    "        try:\n",
    "            txt = tweetTextHandler.del_url(txt)\n",
    "            for token in tknzr.tokenize(txt):\n",
    "                #token = porter.stem(token.lower())\n",
    "                if token.isdigit():\n",
    "                    pass\n",
    "                elif token in stop_words:\n",
    "                    stopwordDict[token] = stopwordDict.get(token, 0) + 1\n",
    "                    stopwordFlag = True\n",
    "                else:\n",
    "                    wordDict[token] = wordDict.get(token, 0) + 1\n",
    "            if stopwordFlag:\n",
    "                stopwordTweetCount += 1\n",
    "        except Exception as e:\n",
    "            #print(\"Error message:\", e)\n",
    "            pass\n",
    "    return wordDict, stopwordDict, stopwordTweetCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patients = dict()\n",
    "ordinarys = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('../0_dataset/patient_ids') as r:\n",
    "    for patient in r.readlines()[:100]:\n",
    "        patient = patient.strip()\n",
    "        patients[patient] = peopleInfo.Patient(patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove users:['181095972', '2409775369', '363595479', '2382824556', '113666708', '2195175613', '402683346', '3031516391', '1229103648']\n"
     ]
    }
   ],
   "source": [
    "patients = filterMethods.by_tweets_number(patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('../0_dataset/ordinary_ids') as r:\n",
    "    for ordinary in r.readlines()[:100]:\n",
    "        ordinary = ordinary.strip()\n",
    "        ordinarys[ordinary] = peopleInfo.Ordinary(ordinary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove users:[]\n"
     ]
    }
   ],
   "source": [
    "ordinarys = filterMethods.by_tweets_number(ordinarys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import plotly\n",
    "# import plotly.offline as offline\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "# from IPython.display import display\n",
    "plotly.tools.set_credentials_file(username='Adeline', api_key='Z5eltNtBQXqvI05ZFQtz')\n",
    "# offline.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total3 = [0, 0, 0]\n",
    "for key in patients:\n",
    "    _w, _s, _sc = getWordsDict(patients[key].getText())\n",
    "    total3[0] += sum(_s.values())\n",
    "    total3[1] += _sc\n",
    "    total3[2] += len(patients[key].inRangeDf)\n",
    "    \n",
    "# 0:Stopwords總數/ 1:含有stopwords的推文總數/ 2:（時間內的）推文總數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total4 = [0, 0, 0]\n",
    "for key in ordinarys:\n",
    "    _w, _s, _sc = getWordsDict(ordinarys[key].getText())\n",
    "    total4[0] += sum(_s.values())\n",
    "    total4[1] += _sc\n",
    "    total4[2] += len(ordinarys[key].inRangeDf)\n",
    "\n",
    "# 0:Stopwords總數/ 1:含有stopwords的推文總數/ 2:（時間內的）推文總數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trace_patients = go.Bar(x=['Type1', 'Type2'],\n",
    "                  y=[round(total3[0]/total3[1],1), round(total3[1]/total3[2],1)],\n",
    "                  name='Patients',\n",
    "                  marker=dict(color='#ffcdd2'))\n",
    "\n",
    "trace_ordinarys = go.Bar(x=['Type1', 'Type2'],\n",
    "                y=[round(total4[0]/total4[1],1), round(total4[1]/total4[2],1)],\n",
    "                name='Ordinarys',\n",
    "                marker=dict(color='#A2D5F2'))\n",
    "data = [trace_patients, trace_ordinarys]\n",
    "\n",
    "layout = go.Layout(title='比較',\n",
    "                xaxis=dict(title='使用者類型'),\n",
    "                yaxis=dict(title='平均數量'))\n",
    "\n",
    "fig3 = go.Figure(data=data, layout=layout)\n",
    "py.image.save_as(fig3, filename='img/a-simple-plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tweet(txt):\n",
    "    \n",
    "    try:\n",
    "        # url\n",
    "        txt = re.sub(r\"http\\S+\", \"\", txt)\n",
    "        txt = re.sub(r\"https\\S+\", \"\", txt)\n",
    "        txt = re.sub(r\"twitter.com/\\S+\", \"\", txt)\n",
    "        txt = re.sub(r\"pic.twitter.com\\S+\", \"\", txt)\n",
    "        txt = re.sub(r\"\\S+/\\S+\",\"\", txt)\n",
    "        #txt = re.sub(r\"\\S+\\/status\\S+\", txt)\n",
    "        # mention\n",
    "        txt = re.sub(r\"@\\S+\", \"\", txt)\n",
    "    except TypeError as e:\n",
    "        print(e)\n",
    "    \n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_texts = []\n",
    "group_texts = []\n",
    "\n",
    "for key in ordinarys.keys():\n",
    "    base_texts.append(filter_tweet('\\n'.join(ordinarys[key].getText())))\n",
    "\n",
    "for key in patients.keys():\n",
    "    group_texts.append(filter_tweet('\\n'.join(patients[key].getText())))\n",
    "\n",
    "corpus = base_texts + group_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=128, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=\"english\",ngram_range = (1,2), min_df=1)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "idf = vectorizer.idf_\n",
    "Y = np.array([0] * len(base_texts) + [1]*len(group_texts), dtype=int)\n",
    "classifier  = RandomForestClassifier(n_jobs=-1, max_features=\"sqrt\", n_estimators=128)\n",
    "classifier.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_indicies = np.argsort(classifier.feature_importances_)\n",
    "vector2word = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = []\n",
    "for i in range(50):\n",
    "    vector = feature_indicies[-i-1]\n",
    "    word = vector2word[vector]\n",
    "    top_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['depression',\n",
       " 'diagnosed',\n",
       " 'sweet',\n",
       " 'extremely',\n",
       " 'hopefully',\n",
       " 've seen',\n",
       " 'like',\n",
       " 'pic',\n",
       " 'wouldn',\n",
       " 'anxiety',\n",
       " 'forever',\n",
       " 'fucking',\n",
       " 'dead',\n",
       " 'things',\n",
       " 'newprofilepic pic',\n",
       " 'attention',\n",
       " 'anymore',\n",
       " 'life',\n",
       " 'read',\n",
       " 'doctor',\n",
       " 'minutes',\n",
       " 'mom',\n",
       " 'lost',\n",
       " 'kid',\n",
       " 'google',\n",
       " 'pic need',\n",
       " 'just really',\n",
       " 'did',\n",
       " 'better',\n",
       " 'watch',\n",
       " 'alive',\n",
       " 'love pic',\n",
       " 'world',\n",
       " 'book',\n",
       " 'hearing',\n",
       " 'probably',\n",
       " 'gets',\n",
       " 'job',\n",
       " 'tho',\n",
       " 'hours',\n",
       " 'good morning',\n",
       " 'important',\n",
       " 'getting',\n",
       " 'diagnosed depression',\n",
       " 'happiness',\n",
       " 'brain',\n",
       " 'feeling like',\n",
       " 'wish',\n",
       " 'care',\n",
       " 'months']"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning:\n",
      "\n",
      "This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(Y, 5, random_state=randint(0,65536))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StratifiedShuffleSplit(labels=[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
       " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
       " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
       " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
       " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
       " 1 1 1 1 1 1], n_iter=5, test_size=0.1, random_state=61006)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions = []\n",
    "for train_index, test_index in sss:\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "    classifier.fit(X_train, Y_train)\n",
    "    score = classifier.score(X_test, Y_test)\n",
    "    precisions.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64000000000000001"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
