{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, math\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from random import randint\n",
    "import ipynb.fs.defs.PeopleInfo as peopleInfo\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import TweetTokenizer\n",
    "from nltk import PorterStemmer\n",
    "# import ipynb.fs.defs.TweetTextHandler as tweetTextHandler\n",
    "import ipynb.fs.defs.FilterMethods as filterMethods\n",
    "import sys, os\n",
    "sys.path.append('../2_feature')\n",
    "import ipynb.fs.defs.GetFeatures as getFeatures\n",
    "global stop_words\n",
    "stop_words = stopwords.words('english')\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from IPython.display import display, Image\n",
    "plotly.tools.set_credentials_file(username='Adeline', api_key='Z5eltNtBQXqvI05ZFQtz')\n",
    "# import plotly.offline as offline\n",
    "# offline.init_notebook_mode(connected=True)\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_by_symbols(txt):\n",
    "    txt = re.sub(r\"https\\S+\", '', txt)\n",
    "    txt = re.sub(r\"http\\S+\", '', txt)\n",
    "    txt = re.sub(r\"pic.twitter.com\\S+\", '', txt)\n",
    "    txt = re.sub(r\"twitter.com/\\S+\", '', txt)\n",
    "    txt = re.sub(r\"\\S+/\\S+\", '', txt)\n",
    "    txt = re.sub(r\"@\\S+\", '', txt)\n",
    "    txt = re.sub(r\"#\\S+\", '', txt)\n",
    "    txt = re.sub(r\"idk\", 'i do not know', txt)   # idk: i don't know\n",
    "    txt = re.sub(r\"tho\", 'though', txt)   # tho\n",
    "    txt = re.sub(r\"i\\'m\", 'i am', txt)\n",
    "    txt = re.sub(r\"you\\'re\", 'you are', txt)\n",
    "    txt = re.sub(r\"he\\'s\", 'he is', txt)\n",
    "    txt = re.sub(r\"she\\'s\", 'she is', txt)\n",
    "    txt = re.sub(r\"it\\'s\", 'it is', txt)\n",
    "    txt = re.sub(r\"we\\'re\", 'we are', txt)\n",
    "    txt = re.sub(r\"they\\'re\", 'they are', txt)\n",
    "    txt = re.sub(r\"isn\\'t\", 'is not', txt)\n",
    "    txt = re.sub(r\"don\\'t\", 'do not', txt)\n",
    "    txt = re.sub(r\"doesn\\'t\", 'does not', txt)\n",
    "    txt = re.sub(r\"didn\\'t\", 'did not', txt)\n",
    "    txt = re.sub(r\"wasn\\'t\", 'was not', txt)\n",
    "    txt = re.sub(r\"weren\\'t\", 'were not', txt)\n",
    "    txt = re.sub(r\"haven\\'t\", 'have not', txt)\n",
    "    txt = re.sub(r\"can\\'t\", 'can not', txt)\n",
    "    txt = re.sub(r\"couldn\\'t\", 'could not', txt)\n",
    "    txt = re.sub(r\"wouldn\\'t\", 'would not', txt)\n",
    "    txt = re.sub(r\"shouldn\\'t\", 'should not', txt)\n",
    "    txt = re.sub(r\"&amp\", '', txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text_list = re.findall('(?u)\\\\b[a-zA-Z]\\\\w{0,}\\\\b', text)\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read LIWC Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_category_dict = dict()\n",
    "liwc_word_dict = dict()\n",
    "with open('/home/adeline/Documents/Depression_Research/LIWC2007 Documents/Dictionaries/LIWC2007_English080730.dic') as open_file:\n",
    "    raw_data = open_file.readlines()\n",
    "    for row in raw_data[1:65]:\n",
    "        row = row.strip().split('\\t')\n",
    "        liwc_category_dict[row[0]] = row[1]\n",
    "    for row in raw_data[66:4553]:\n",
    "        row = row.strip().split('\\t')\n",
    "        if row[0] in stop_words:\n",
    "            liwc_word_dict[row[0]] = row[1:]\n",
    "        elif '*' in row[0] and row[0][:-1] in stop_words:\n",
    "            liwc_word_dict[row[0]] = row[1:]\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_word_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patients = dict()\n",
    "ordinarys = dict()\n",
    "with open('../0_dataset/patient_ids') as r:\n",
    "    for patient in r.readlines()[:100]:\n",
    "        patient = patient.strip()\n",
    "        patients[patient] = peopleInfo.Patient(patient)\n",
    "with open('../0_dataset/ordinary_ids') as r:\n",
    "    for ordinary in r.readlines()[:100]:\n",
    "        ordinary = ordinary.strip()\n",
    "        ordinarys[ordinary] = peopleInfo.Ordinary(ordinary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove users:[]\n",
      "Remove users:[]\n"
     ]
    }
   ],
   "source": [
    "patients = filterMethods.filter_user_by_tweet_number(patients)\n",
    "ordinarys = filterMethods.filter_user_by_tweet_number(ordinarys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_texts = []\n",
    "group_texts = []\n",
    "\n",
    "for key in patients.keys():\n",
    "    group_texts.append(replace_by_symbols('\\n'.join(patients[key].getText())))\n",
    "\n",
    "for key in ordinarys.keys():\n",
    "    base_texts.append(replace_by_symbols('\\n'.join(ordinarys[key].getText())))\n",
    "\n",
    "corpus = base_texts + group_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Data Statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_filter(df):\n",
    "    # 過濾空列\n",
    "    filter = df['Text'] != ''\n",
    "    df = df[filter]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_tweets = []    # one element presented one tweet\n",
    "group_tweets = []\n",
    "for line in group_texts:\n",
    "    group_tweets.extend(line.split('\\n'))\n",
    "for line in base_texts:\n",
    "    base_tweets.extend(line.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet_group_len = [len(tokenize(x)) for x in group_tweets]\n",
    "# tweet_base_len = [len(tokenize(x)) for x in base_tweets]\n",
    "tweet_group_len = [len(x.split()) for x in group_tweets]\n",
    "tweet_base_len = [len(x.split()) for x in base_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfGroupTweets = df_filter(pd.DataFrame({'Text':group_tweets,'len':tweet_group_len}))\n",
    "dfBaseTweets = df_filter(pd.DataFrame({'Text':base_tweets,'len':tweet_base_len}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Brief Info of Group Tweets ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    99668.000000\n",
       "mean        11.514328\n",
       "std          8.246411\n",
       "min          0.000000\n",
       "25%          4.000000\n",
       "50%         10.000000\n",
       "75%         18.000000\n",
       "max         42.000000\n",
       "Name: len, dtype: float64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('=== Brief Info of Group Tweets ===')\n",
    "dfGroupTweets['len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Brief Info of Base Tweets ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    11908.000000\n",
       "mean        10.626386\n",
       "std          6.390020\n",
       "min          0.000000\n",
       "25%          5.000000\n",
       "50%          9.000000\n",
       "75%         15.000000\n",
       "max         33.000000\n",
       "Name: len, dtype: float64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('=== Brief Info of Base Tweets ===')\n",
    "dfBaseTweets['len'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_stopwrods_dict(text_list, mydict):\n",
    "\n",
    "    position = []\n",
    "    for i, text in enumerate(text_list):\n",
    "        if text in stop_words:    # 是個 stopword\n",
    "            position.append(i)\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    for i in range(len(position)):\n",
    "        try:\n",
    "            cur_pos = position[i]\n",
    "            next_pos = position[i+1]\n",
    "            key = '{0}-{1}-{2}'.format(text_list[cur_pos], text_list[next_pos], str(next_pos-cur_pos))\n",
    "            mydict[key] = mydict.get(key, 0) + 1\n",
    "        except IndexError:\n",
    "            continue\n",
    "    \n",
    "    \n",
    "    return mydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mydict = dict()\n",
    "for texts in group_texts:\n",
    "    for text in texts.split('\\n'):\n",
    "        construct_stopwrods_dict(tokenize(text), mydict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mydict2 = dict()\n",
    "for texts in base_texts:\n",
    "    for text in texts.split('\\n'):\n",
    "        construct_stopwrods_dict(tokenize(text), mydict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'mydict' (dict) to file 'stopwords_dict_depression'.\n"
     ]
    }
   ],
   "source": [
    "%store mydict >> stopwords_dict_depression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'mydict2' (dict) to file 'stopwords_dict_ordinary'.\n"
     ]
    }
   ],
   "source": [
    "%store mydict2 >> stopwords_dict_ordinary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intersect of Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "intersect = mydict.keys() & mydict2.keys()   # 聯集的 Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difference of Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffsetGroup = mydict.keys() - mydict2.keys()    # 差集（Group 的 key）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diffsetBase = mydict2.keys() - mydict2.keys()    # 差集（Base 的 key）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of set belongs to Group: 25014\tBase: 0\n"
     ]
    }
   ],
   "source": [
    "print('The length of set belongs to Group: {0}\\tBase: {1}'.format(len(diffsetGroup),len(diffsetBase)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_num_distribution(mydict):\n",
    "    keys = list(mydict.keys())\n",
    "    vals = list(mydict.values())\n",
    "    trace1 = go.Bar(\n",
    "        x=keys,\n",
    "        y=vals,\n",
    "        name='',\n",
    "        marker=dict(color='rgb(49,130,189)'))\n",
    "    data = [trace1]\n",
    "    layout = go.Layout(\n",
    "        title = 'Stopwords Difference Set',\n",
    "#         shapes = [{\n",
    "#             'type': 'line',\n",
    "#             'x0': 5,\n",
    "#             'y0': 0,\n",
    "#             'x1': 5,\n",
    "#             'y1': 6000,\n",
    "#             'line': {\n",
    "#                 'color': 'red',\n",
    "#                 'width': 1,\n",
    "#                 'dash':'dashdot'\n",
    "#             }\n",
    "#         },\n",
    "#         {\n",
    "#             'type': 'line',\n",
    "#             'x0': 8,\n",
    "#             'y0': 0,\n",
    "#             'x1': 8,\n",
    "#             'y1': 6000,\n",
    "#             'line': {\n",
    "#                 'color': 'red',\n",
    "#                 'width': 1,\n",
    "#                 'dash':'dashdot'\n",
    "#             },\n",
    "#         }]\n",
    "    )\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    return fig\n",
    "    #     py.iplot(fig, filename='stwdiff')\n",
    "    \n",
    "    \n",
    "def create_num_distribution(key_list, original_dict):\n",
    "    new_dict = dict()\n",
    "    for key in key_list:\n",
    "        new_dict[key] = original_dict[key]\n",
    "    \n",
    "    num_dict = dict()\n",
    "    for key in key_list:\n",
    "        num = new_dict[key]\n",
    "        num_dict[num] = dist.get(num, 0) + 1\n",
    "    \n",
    "    return num_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Adeline/206.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py.iplot(plot_num_distribution(create_num_distribution(diffsetGroup, mydict)), filename='StopwordsDiffGroup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupFeatureOrigin = dict()\n",
    "for key in diffset:\n",
    "    if 4 < mydict[key] > 8:\n",
    "        groupFeatureOrigin[key] = mydict[key]\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeToFileBasic(data, filename):\n",
    "    print(type(data))\n",
    "    if type(data) == dict:\n",
    "        print('Writing dictionary to {0}...'.format(filename))\n",
    "        with open(filename, 'a') as open_file:\n",
    "            for key, val in data.items():\n",
    "                open_file.write('{0}\\t{1}\\n'.format(key, val))\n",
    "    elif type(data) == list:\n",
    "        print('Writing list to {0}...'.format(filename))\n",
    "        with open(filename, 'a') as open_file:\n",
    "            for val in data:\n",
    "                open_file.write('{0}\\n'.format(val))\n",
    "    else:\n",
    "        print('InputFileTypeError')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dictionary to groupFeatureOrigin...\n"
     ]
    }
   ],
   "source": [
    "writeToFileBasic(groupFeatureOrigin, 'groupFeatureOrigin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Adeline/208.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py.iplot(plot_num_distribution(create_num_distribution(diffsetBase, mydict2)), filename='StopwordsDiffBase')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Pattern List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convertToPatternList(feature_list):\n",
    "    %%time\n",
    "    regStrs = []\n",
    "    for line in feature_list:\n",
    "        items = line.strip().split('\\t')\n",
    "        keys = items[0].split('-')\n",
    "        regStr = ''\n",
    "        tmpRegStr1 = ''\n",
    "        tmpRegStr2 = ''\n",
    "        tmpRegStr3 = ''\n",
    "        tmpRegStr4 = ''\n",
    "        regGeneralStr = ''\n",
    "        # General part\n",
    "        for i in range(int(keys[2])-1):\n",
    "            regGeneralStr += '\\s\\w+'\n",
    "        regGeneralStr += '\\s'\n",
    "        # C1: 'a b'\n",
    "        tmpRegStr1 = '^' + keys[0] +  regGeneralStr + keys[1] + '$'\n",
    "        # C2: 'a b '\n",
    "        tmpRegStr2 = '^' + keys[0] + regGeneralStr + keys[1] + '\\s'\n",
    "        # C3: ' a b'\n",
    "        tmpRegStr3 = '\\s' + keys[0] + regGeneralStr + keys[1] + '$'\n",
    "        # C4: ' a b '\n",
    "        tmpRegStr4 = '\\s' + keys[0] + regGeneralStr + keys[1] + '\\s'\n",
    "        regStr = '('+tmpRegStr1+')|('+tmpRegStr2+')|('+tmpRegStr3+')|('+tmpRegStr4+')'\n",
    "        regStrs.append(regStr)\n",
    "    return regStrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 7.63 µs\n",
      "<class 'list'>\n",
      "Writing list to groupFeatureOriginPatternList...\n"
     ]
    }
   ],
   "source": [
    "writeToFileBasic(convertToPatternList(groupFeatureOrigin), 'groupFeatureOriginPatternList')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 7.63 µs\n"
     ]
    }
   ],
   "source": [
    "regStrs = convertToPatternList(groupFeatureOrigin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHitNumber(text):\n",
    "    \n",
    "    hit = 0               # 有符合 Pattern 的數量\n",
    "    hitTweetNum = 0       # 有符合 Pattern 的發文數量\n",
    "    tweetLengthNum = 0    # 還沒想到 Length 怎麼運用\n",
    "    tweetNum = 0          # 全部的發文數量\n",
    "\n",
    "    text_list = text.split('\\n')  # 把一個user原本以\\n串連的發文分開\n",
    "    tweetNum = len(text_list)\n",
    "    print(tweetNum)\n",
    "    for tweet in text_list:\n",
    "        hitFlag = False\n",
    "        tknzdTweet = tokenize(tweet)\n",
    "        tweetLengthNum += len(tknzdTweet)\n",
    "        tweet = \" \".join(tknzdTweet)\n",
    "        for regStr in regStrs:\n",
    "            if re.search(re.compile(regStr), tweet) != None:\n",
    "                hit += 1\n",
    "                if hitFlag == False:\n",
    "                    hitTweetNum += 1\n",
    "                    hitFlag = True\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "    return {'hit':hit, 'hitTweetNum':hitTweetNum, 'tweetLengthNum':tweetLengthNum, 'tweetNum':tweetNum}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f912fe5e7d04cec9808581cd1f346e4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "1616\n",
      "1485\n",
      "482\n",
      "491\n",
      "1988\n",
      "424\n",
      "45\n",
      "4700\n",
      "227\n",
      "561\n",
      "431\n",
      "38\n",
      "21\n",
      "157\n",
      "110\n",
      "152\n",
      "5552\n",
      "1323\n",
      "802\n",
      "1174\n",
      "532\n",
      "946\n",
      "66\n",
      "82\n",
      "494\n",
      "909\n",
      "10070\n",
      "1304\n",
      "2071\n",
      "49\n",
      "1173\n",
      "309\n",
      "1387\n",
      "486\n",
      "731\n",
      "637\n",
      "1671\n",
      "444\n",
      "3971\n",
      "71\n",
      "275\n",
      "1350\n",
      "14\n",
      "1145\n",
      "2429\n",
      "1413\n",
      "334\n",
      "973\n",
      "712\n",
      "2132\n",
      "62\n",
      "46\n",
      "460\n",
      "102\n",
      "134\n",
      "86\n",
      "413\n",
      "288\n",
      "89\n",
      "2191\n",
      "3773\n",
      "2697\n",
      "182\n",
      "222\n",
      "1182\n",
      "645\n",
      "23\n",
      "509\n",
      "813\n",
      "2107\n",
      "20\n",
      "2724\n",
      "1265\n",
      "70\n",
      "5268\n",
      "959\n",
      "767\n",
      "57\n",
      "129\n",
      "2664\n",
      "207\n",
      "1377\n",
      "865\n",
      "638\n",
      "18\n",
      "5075\n",
      "338\n",
      "2998\n",
      "CPU times: user 11h 39min 46s, sys: 1.12 s, total: 11h 39min 47s\n",
      "Wall time: 11h 39min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "avgHitCatNum = {'Group':[], 'Base':[]}\n",
    "avgHitTweetNum = {'Group':[], 'Base':[]}\n",
    "\n",
    "progress = IntProgress()\n",
    "progress.max = len(group_texts)\n",
    "progress.description = '(Init)'\n",
    "display(progress)\n",
    "\n",
    "for i, person in enumerate(group_texts):\n",
    "    progress.value += 1\n",
    "    progress.description = 'Task'+str(i+1)\n",
    "    mydict = getHitNumber(person)\n",
    "#     writeToFileBasic(mydict,'groupPretrainModule')\n",
    "    avgHitCatNum['Group'].append(round(mydict['hit']/mydict['hitTweetNum'],2))\n",
    "    avgHitTweetNum['Group'].append(round(mydict['hitTweetNum']/mydict['tweetNum'],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=128, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(np.reshape([0]*100 + avgHitCatNum['Group'],(-1,1)))\n",
    "Y = np.array([0]*100 + [1]*len(avgHitCatNum['Group']), dtype=int)\n",
    "classifier = RandomForestClassifier(n_jobs=-1, max_features=\"sqrt\", n_estimators=128)\n",
    "classifier.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(Y, 10, random_state=randint(0,65536))\n",
    "classifier = RandomForestClassifier(n_jobs=-1, max_features=\"sqrt\", n_estimators=128)\n",
    "        \n",
    "precisions = []\n",
    "for train_index, test_index in sss:\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "    classifier = classifier.fit(X_train, Y_train)\n",
    "    score = classifier.score(X_test, Y_test)\n",
    "    precisions.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer original patteren to category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictWithCatName = {'Group':{}, 'Base':{}}\n",
    "with open('stopwords_dict_depression') as open_file:\n",
    "    for line in open_file:\n",
    "        eles = re.split(\"[\\t-]+\", line.strip())\n",
    "        try:\n",
    "            key1 = liwc_category_dict[liwc_word_dict[eles[0]][0]]\n",
    "            key2 = liwc_category_dict[liwc_word_dict[eles[1]][0]]\n",
    "            key = '{0}-{1}-{2}'.format(key1, key2, eles[2])\n",
    "            val = eles[3]\n",
    "            dictWithCatName['Group'][key] = dictWithCatName['Group'].get(key, 0) + 1\n",
    "        except KeyError: # 在 LIWC 裡找不到對應的 Key\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('stopwords_dict_ordinary') as open_file:\n",
    "    for line in open_file:\n",
    "        eles = re.split(\"[\\t-]+\", line.strip())\n",
    "        try:\n",
    "            key1 = liwc_category_dict[liwc_word_dict[eles[0]][0]]\n",
    "            key2 = liwc_category_dict[liwc_word_dict[eles[1]][0]]\n",
    "            key = '{0}-{1}-{2}'.format(key1, key2, eles[2])\n",
    "            val = eles[3]\n",
    "            dictWithCatName['Base'][key] = dictWithCatName['Base'].get(key, 0) + 1\n",
    "        except KeyError: # 在 LIWC 裡找不到對應的 Key\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "setWithCatName = dictWithCatName['Group'].keys() - dictWithCatName['Base'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHitNumberCategory(text):\n",
    "    catTableGroupAndBase, catTableGroup, catTableBase = [], [], []\n",
    "    with open('categoryTableGroupAndBase') as open_file:\n",
    "        for line in open_file.readlines():\n",
    "            catTableGroupAndBase.append(line.strip().split('\\t')[1])\n",
    "            \n",
    "    with open('categoryTableGroup') as open_file:\n",
    "        for line in open_file.readlines():\n",
    "            catTableGroup.append(line.strip().split('\\t')[1])\n",
    "    \n",
    "    with open('categoryTableBase') as open_file:\n",
    "        for line in open_file.readlines():\n",
    "            catTableBase.append(line.strip().split('\\t')[1])\n",
    "    \n",
    "    hitGroupAndBase = 0   # 有符合 Pattern 的數量\n",
    "    hitGroup = 0\n",
    "    hitBase = 0\n",
    "    hitTweetNum = 0       # 有符合 Pattern 的發文數量\n",
    "    tweetLengthNum = 0    # 還沒想到 Length 怎麼運用\n",
    "    tweetNum = 0          # 全部的發文數量\n",
    "\n",
    "    text_list = text.split('\\n')  # 把一個user原本以\\n串連的發文分開\n",
    "    tweetNum = len(text_list)\n",
    "#     print(tweetNum)\n",
    "    for tweet in text_list:\n",
    "        hitFlag = False\n",
    "        tknzdTweet = tokenize(tweet)\n",
    "        tweetLengthNum += len(tknzdTweet)\n",
    "        pos = []\n",
    "        smallCat = []\n",
    "        for i, ele in enumerate(tknzdTweet):\n",
    "            if ele in stop_words:\n",
    "                pos.append(i)\n",
    "                try:\n",
    "                    smallCat.append(liwc_word_dict[ele][0])\n",
    "                except KeyError:\n",
    "                    continue\n",
    "        \n",
    "        patternList = []\n",
    "        for i in range(len(pos)):\n",
    "            try:\n",
    "                tmpStr = '{0}-{1}-{2}'.format(smallCat[i], smallCat[i+1], pos[i+1]-pos[i])\n",
    "            except IndexError:\n",
    "                continue\n",
    "            if tmpStr in catTableGroupAndBase:\n",
    "                hitGroupAndBase += 1\n",
    "                if hitFlag == False:\n",
    "                    hitTweetNum += 1\n",
    "                    hitFlag = True\n",
    "            if tmpStr in catTableGroup:\n",
    "                hitGroup += 1\n",
    "                if hitFlag == False:\n",
    "                    hitTweetNum += 1\n",
    "                    hitFlag = True\n",
    "            if tmpStr in catTableBase:\n",
    "                hitBase += 1\n",
    "                if hitFlag == False:\n",
    "                    hitTweetNum += 1\n",
    "                    hitFlag = True\n",
    "                \n",
    "    return {'hitGroupAndBase':hitGroupAndBase, 'hitGroup':hitGroup, 'hitBase':hitBase, 'hitTweetNum':hitTweetNum, 'tweetLengthNum':tweetLengthNum, 'tweetNum':tweetNum}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avgHitCatNumGroupAndBase = {'Group':[], 'Base':[]}\n",
    "avgHitCatNumGroup = {'Group':[], 'Base':[]}\n",
    "avgHitCatNumBase = {'Group':[], 'Base':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb26e6a00dc34152b3351f970fb63983"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.63 s, sys: 8 ms, total: 3.64 s\n",
      "Wall time: 3.65 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "progress = IntProgress()\n",
    "progress.max = len(group_texts)\n",
    "progress.description = '(Init)'\n",
    "display(progress)\n",
    "\n",
    "with open('threeCategoriesHitNumber', 'a') as open_file:    # 記錄每個使用者三個種類的分數\n",
    "    for i, person in enumerate(group_texts):\n",
    "        progress.value += 1\n",
    "        progress.description = 'Task'+str(i+1)\n",
    "        mydict = getHitNumberCategory(person)\n",
    "        open_file.write('{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\n'.format(mydict['hitGroupAndBase'],mydict['hitGroup'],mydict['hitBase'],mydict['hitTweetNum'],mydict['hitTweetNum'],mydict['tweetLengthNum'],mydict['tweetNum']))\n",
    "        try:\n",
    "            avgHitCatNumGroupAndBase['Group'].append(round(mydict['hitGroupAndBase']/mydict['hitTweetNum'],2))\n",
    "        except ZeroDivisionError:\n",
    "            avgHitCatNumGroupAndBase['Group'].append(0)\n",
    "        try:\n",
    "            avgHitCatNumGroup['Group'].append(mydict['hitGroup']/mydict['hitTweetNum'])\n",
    "        except ZeroDivisionError:\n",
    "            avgHitCatNumGroup['Group'].append(0)\n",
    "        try:\n",
    "            avgHitCatNumBase['Group'].append(mydict['hitBase']/mydict['hitTweetNum'])\n",
    "        except ZeroDivisionError:\n",
    "            avgHitCatNumBase['Group'].append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57c9bd9c3a0541aca993d8259e9e08b1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 568 ms, sys: 12 ms, total: 580 ms\n",
      "Wall time: 574 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "progress = IntProgress()\n",
    "progress.max = len(group_texts)\n",
    "progress.description = '(Init)'\n",
    "display(progress)\n",
    "\n",
    "with open('threeCategoriesHitNumber', 'a') as open_file:    # 記錄每個使用者三個種類的分數\n",
    "    for i, person in enumerate(base_texts):\n",
    "        progress.value += 1\n",
    "        progress.description = 'Task'+str(i+1)\n",
    "        mydict = getHitNumberCategory(person)\n",
    "        open_file.write('{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\n'.format(mydict['hitGroupAndBase'],mydict['hitGroup'],mydict['hitBase'],mydict['hitTweetNum'],mydict['hitTweetNum'],mydict['tweetLengthNum'],mydict['tweetNum']))\n",
    "        try:\n",
    "            avgHitCatNumGroupAndBase['Base'].append(round(mydict['hitGroupAndBase']/mydict['hitTweetNum'],2))\n",
    "        except ZeroDivisionError:\n",
    "            avgHitCatNumGroupAndBase['Base'].append(0)\n",
    "        try:\n",
    "            avgHitCatNumGroup['Base'].append(mydict['hitGroup']/mydict['hitTweetNum'])\n",
    "        except ZeroDivisionError:\n",
    "            avgHitCatNumGroup['Base'].append(0)\n",
    "        try:\n",
    "            avgHitCatNumBase['Base'].append(mydict['hitBase']/mydict['hitTweetNum'])\n",
    "        except ZeroDivisionError:\n",
    "            avgHitCatNumBase['Base'].append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scoresDepression = []\n",
    "scoresOrdinary = []\n",
    "with open('patterListFeatures') as open_file:\n",
    "    lines = open_file.readlines()\n",
    "    for line in lines[:89]:\n",
    "        line = line.strip().split('\\t')\n",
    "        scoresDepression.append(line[0::2])\n",
    "        scoresOrdinary.append(line[1::2])\n",
    "    for line in lines[89:]:\n",
    "        line = line.strip().split('\\t')\n",
    "        scoresOrdinary.append(line[::2])\n",
    "        \n",
    "X = np.array(scoresOrdinary+scoresDepression)\n",
    "Y = np.array([0]*100 + [1]*89, dtype=int)\n",
    "\n",
    "sss = StratifiedShuffleSplit(Y, 10, random_state=randint(0,65536))\n",
    "classifier = RandomForestClassifier(n_jobs=-1, max_features=\"sqrt\", n_estimators=128)\n",
    "        \n",
    "precisions = []\n",
    "for train_index, test_index in sss:\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "    classifier = classifier.fit(X_train, Y_train)\n",
    "    score = classifier.score(X_test, Y_test)\n",
    "    precisions.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(scoresOrdinary+scoresDepression)\n",
    "Y = np.array([0]*100 + [1]*89, dtype=int)\n",
    "\n",
    "sss = StratifiedShuffleSplit(Y, 10, random_state=randint(0,65536))\n",
    "classifier = RandomForestClassifier(n_jobs=-1, max_features=\"sqrt\", n_estimators=128)\n",
    "        \n",
    "precisions = []\n",
    "for train_index, test_index in sss:\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "    classifier = classifier.fit(X_train, Y_train)\n",
    "    score = classifier.score(X_test, Y_test)\n",
    "    precisions.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
