{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import re, math\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from random import randint\n",
    "import ipynb.fs.defs.PeopleInfo as peopleInfo\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import TweetTokenizer\n",
    "from nltk import PorterStemmer\n",
    "# import ipynb.fs.defs.TweetTextHandler as tweetTextHandler\n",
    "import ipynb.fs.defs.FilterMethods as filterMethods\n",
    "import sys, os\n",
    "sys.path.append('../2_feature')\n",
    "import ipynb.fs.defs.GetFeatures as getFeatures\n",
    "global stop_words\n",
    "stop_words = stopwords.words('english')\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from IPython.display import display, Image\n",
    "plotly.tools.set_credentials_file(username='Adeline', api_key='Z5eltNtBQXqvI05ZFQtz')\n",
    "# import plotly.offline as offline\n",
    "# offline.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_by_symbols(txt):\n",
    "    txt = re.sub(r\"https\\S+\", '', txt)\n",
    "    txt = re.sub(r\"http\\S+\", '', txt)\n",
    "    txt = re.sub(r\"pic.twitter.com\\S+\", '', txt)\n",
    "    txt = re.sub(r\"twitter.com/\\S+\", '', txt)\n",
    "    txt = re.sub(r\"\\S+/\\S+\", '', txt)\n",
    "    txt = re.sub(r\"@\\S+\", '', txt)\n",
    "    txt = re.sub(r\"#\\S+\", '', txt)\n",
    "    txt = re.sub(r\"idk\", 'i do not know', txt)   # idk: i don't know\n",
    "    txt = re.sub(r\"tho\", 'though', txt)   # tho\n",
    "    txt = re.sub(r\"i\\'m\", 'i am', txt)\n",
    "    txt = re.sub(r\"you\\'re\", 'you are', txt)\n",
    "    txt = re.sub(r\"he\\'s\", 'he is', txt)\n",
    "    txt = re.sub(r\"she\\'s\", 'she is', txt)\n",
    "    txt = re.sub(r\"it\\'s\", 'it is', txt)\n",
    "    txt = re.sub(r\"we\\'re\", 'we are', txt)\n",
    "    txt = re.sub(r\"they\\'re\", 'they are', txt)\n",
    "    txt = re.sub(r\"isn\\'t\", 'is not', txt)\n",
    "    txt = re.sub(r\"don\\'t\", 'do not', txt)\n",
    "    txt = re.sub(r\"doesn\\'t\", 'does not', txt)\n",
    "    txt = re.sub(r\"didn\\'t\", 'did not', txt)\n",
    "    txt = re.sub(r\"wasn\\'t\", 'was not', txt)\n",
    "    txt = re.sub(r\"weren\\'t\", 'were not', txt)\n",
    "    txt = re.sub(r\"haven\\'t\", 'have not', txt)\n",
    "    txt = re.sub(r\"can\\'t\", 'can not', txt)\n",
    "    txt = re.sub(r\"couldn\\'t\", 'could not', txt)\n",
    "    txt = re.sub(r\"wouldn\\'t\", 'would not', txt)\n",
    "    txt = re.sub(r\"shouldn\\'t\", 'should not', txt)\n",
    "    txt = re.sub(r\"&amp\", '', txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text_list = re.findall('(?u)\\\\b[a-zA-Z]\\\\w{0,}\\\\b', text)\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read LIWC Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_category_dict = dict()\n",
    "liwc_word_dict = dict()\n",
    "with open('/home/adeline/Documents/Depression_Research/LIWC2007 Documents/Dictionaries/LIWC2007_English080730.dic') as open_file:\n",
    "    raw_data = open_file.readlines()\n",
    "    for row in raw_data[1:65]:\n",
    "        row = row.strip().split('\\t')\n",
    "        liwc_category_dict[row[0]] = row[1]\n",
    "    for row in raw_data[66:4553]:\n",
    "        row = row.strip().split('\\t')\n",
    "        if row[0] in stop_words:\n",
    "            liwc_word_dict[row[0]] = row[1:]\n",
    "        elif '*' in row[0] and row[0][:-1] in stop_words:\n",
    "            liwc_word_dict[row[0]] = row[1:]\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_word_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patients = dict()\n",
    "ordinarys = dict()\n",
    "with open('../0_dataset/patient_ids') as r:\n",
    "    for patient in r.readlines()[:100]:\n",
    "        patient = patient.strip()\n",
    "        patients[patient] = peopleInfo.Patient(patient)\n",
    "with open('../0_dataset/ordinary_ids') as r:\n",
    "    for ordinary in r.readlines()[:100]:\n",
    "        ordinary = ordinary.strip()\n",
    "        ordinarys[ordinary] = peopleInfo.Ordinary(ordinary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove users:[]\n",
      "Remove users:[]\n"
     ]
    }
   ],
   "source": [
    "patients = filterMethods.filter_user_by_tweet_number(patients)\n",
    "ordinarys = filterMethods.filter_user_by_tweet_number(ordinarys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_texts = []\n",
    "group_texts = []\n",
    "\n",
    "for key in patients.keys():\n",
    "    group_texts.append(replace_by_symbols('\\n'.join(patients[key].getText())))\n",
    "\n",
    "for key in ordinarys.keys():\n",
    "    base_texts.append(replace_by_symbols('\\n'.join(ordinarys[key].getText())))\n",
    "\n",
    "corpus = base_texts + group_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Data Statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_filter(df):\n",
    "    # 過濾空列\n",
    "    filter = df['Text'] != ''\n",
    "    df = df[filter]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_tweets = []    # one element presented one tweet\n",
    "group_tweets = []\n",
    "for line in group_texts:\n",
    "    group_tweets.extend(line.split('\\n'))\n",
    "for line in base_texts:\n",
    "    base_tweets.extend(line.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet_group_len = [len(tokenize(x)) for x in group_tweets]\n",
    "# tweet_base_len = [len(tokenize(x)) for x in base_tweets]\n",
    "tweet_group_len = [len(x.split()) for x in group_tweets]\n",
    "tweet_base_len = [len(x.split()) for x in base_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfGroupTweets = df_filter(pd.DataFrame({'Text':group_tweets,'len':tweet_group_len}))\n",
    "dfBaseTweets = df_filter(pd.DataFrame({'Text':base_tweets,'len':tweet_base_len}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Brief Info of Group Tweets ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    99668.000000\n",
       "mean        11.514328\n",
       "std          8.246411\n",
       "min          0.000000\n",
       "25%          4.000000\n",
       "50%         10.000000\n",
       "75%         18.000000\n",
       "max         42.000000\n",
       "Name: len, dtype: float64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('=== Brief Info of Group Tweets ===')\n",
    "dfGroupTweets['len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Brief Info of Base Tweets ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    11908.000000\n",
       "mean        10.626386\n",
       "std          6.390020\n",
       "min          0.000000\n",
       "25%          5.000000\n",
       "50%          9.000000\n",
       "75%         15.000000\n",
       "max         33.000000\n",
       "Name: len, dtype: float64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('=== Brief Info of Base Tweets ===')\n",
    "dfBaseTweets['len'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_stopwrods_dict(text_list, mydict):\n",
    "\n",
    "    position = []\n",
    "    for i, text in enumerate(text_list):\n",
    "        if text in stop_words:    # 是個 stopword\n",
    "            position.append(i)\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    for i in range(len(position)):\n",
    "        try:\n",
    "            cur_pos = position[i]\n",
    "            next_pos = position[i+1]\n",
    "            key = '{0}-{1}-{2}'.format(text_list[cur_pos], text_list[next_pos], str(next_pos-cur_pos))\n",
    "            mydict[key] = mydict.get(key, 0) + 1\n",
    "        except IndexError:\n",
    "            continue\n",
    "    \n",
    "    \n",
    "    return mydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mydict = dict()\n",
    "for texts in group_texts:\n",
    "    for text in texts.split('\\n'):\n",
    "        construct_stopwrods_dict(tokenize(text), mydict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mydict2 = dict()\n",
    "for texts in base_texts:\n",
    "    for text in texts.split('\\n'):\n",
    "        construct_stopwrods_dict(tokenize(text), mydict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'mydict' (dict) to file 'stopwords_dict_depression'.\n"
     ]
    }
   ],
   "source": [
    "%store mydict >> stopwords_dict_depression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'mydict2' (dict) to file 'stopwords_dict_ordinary'.\n"
     ]
    }
   ],
   "source": [
    "%store mydict2 >> stopwords_dict_ordinary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intersect of Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "intersect = mydict.keys() & mydict2.keys()   # 聯集的 Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difference of Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffsetGroup = mydict.keys() - mydict2.keys()    # 差集（Group 的 key）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diffsetBase = mydict2.keys() - mydict2.keys()    # 差集（Base 的 key）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of set belongs to Group: 25014\tBase: 0\n"
     ]
    }
   ],
   "source": [
    "print('The length of set belongs to Group: {0}\\tBase: {1}'.format(len(diffsetGroup),len(diffsetBase)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_num_distribution(mydict):\n",
    "    keys = list(mydict.keys())\n",
    "    vals = list(mydict.values())\n",
    "    trace1 = go.Bar(\n",
    "        x=keys,\n",
    "        y=vals,\n",
    "        name='',\n",
    "        marker=dict(color='rgb(49,130,189)'))\n",
    "    data = [trace1]\n",
    "    layout = go.Layout(\n",
    "        title = 'Stopwords Difference Set',\n",
    "#         shapes = [{\n",
    "#             'type': 'line',\n",
    "#             'x0': 5,\n",
    "#             'y0': 0,\n",
    "#             'x1': 5,\n",
    "#             'y1': 6000,\n",
    "#             'line': {\n",
    "#                 'color': 'red',\n",
    "#                 'width': 1,\n",
    "#                 'dash':'dashdot'\n",
    "#             }\n",
    "#         },\n",
    "#         {\n",
    "#             'type': 'line',\n",
    "#             'x0': 8,\n",
    "#             'y0': 0,\n",
    "#             'x1': 8,\n",
    "#             'y1': 6000,\n",
    "#             'line': {\n",
    "#                 'color': 'red',\n",
    "#                 'width': 1,\n",
    "#                 'dash':'dashdot'\n",
    "#             },\n",
    "#         }]\n",
    "    )\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    return fig\n",
    "    #     py.iplot(fig, filename='stwdiff')\n",
    "    \n",
    "    \n",
    "def create_num_distribution(key_list, original_dict):\n",
    "    new_dict = dict()\n",
    "    for key in key_list:\n",
    "        new_dict[key] = original_dict[key]\n",
    "    \n",
    "    num_dict = dict()\n",
    "    for key in key_list:\n",
    "        num = new_dict[key]\n",
    "        num_dict[num] = dist.get(num, 0) + 1\n",
    "    \n",
    "    return num_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Adeline/206.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py.iplot(plot_num_distribution(create_num_distribution(diffsetGroup, mydict)), filename='StopwordsDiffGroup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupFeatureOrigin = dict()\n",
    "for key in diffset:\n",
    "    if 4 < mydict[key] > 8:\n",
    "        groupFeatureOrigin[key] = mydict[key]\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeToFileBasic(data, filename):\n",
    "    print(type(data))\n",
    "    if type(data) == dict:\n",
    "        print('Writing dictionary to {0}...'.format(filename))\n",
    "        with open(filename, 'w') as open_file:\n",
    "            for key, val in dictionary.items():\n",
    "                open_file.write('{0}\\t{1}\\n'.format(key, val))\n",
    "    elif type(data) == list:\n",
    "        print('Writing list to {0}...'.format(filename))\n",
    "        with open(filename, 'w') as open_file:\n",
    "            for val in data:\n",
    "                open_file.write('{0}\\n'.format(val))\n",
    "    else:\n",
    "        print('InputFileTypeError')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dictionary to groupFeatureOrigin...\n"
     ]
    }
   ],
   "source": [
    "writeToFileBasic(groupFeatureOrigin, 'groupFeatureOrigin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Adeline/208.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py.iplot(plot_num_distribution(create_num_distribution(diffsetBase, mydict2)), filename='StopwordsDiffBase')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Pattern List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convertToPatternList(feature_list):\n",
    "    %%time\n",
    "    regStrs = []\n",
    "    for line in feature_list:\n",
    "        items = line.strip().split('\\t')\n",
    "        keys = items[0].split('-')\n",
    "        regStr = ''\n",
    "        tmpRegStr1 = ''\n",
    "        tmpRegStr2 = ''\n",
    "        tmpRegStr3 = ''\n",
    "        tmpRegStr4 = ''\n",
    "        regGeneralStr = ''\n",
    "        # General part\n",
    "        for i in range(int(keys[2])-1):\n",
    "            regGeneralStr += '\\s\\w+'\n",
    "        regGeneralStr += '\\s'\n",
    "        # C1: 'a b'\n",
    "        tmpRegStr1 = '^' + keys[0] +  regGeneralStr + keys[1] + '$'\n",
    "        # C2: 'a b '\n",
    "        tmpRegStr2 = '^' + keys[0] + regGeneralStr + keys[1] + '\\s'\n",
    "        # C3: ' a b'\n",
    "        tmpRegStr3 = '\\s' + keys[0] + regGeneralStr + keys[1] + '$'\n",
    "        # C4: ' a b '\n",
    "        tmpRegStr4 = '\\s' + keys[0] + regGeneralStr + keys[1] + '\\s'\n",
    "        regStr = '('+tmpRegStr1+')|('+tmpRegStr2+')|('+tmpRegStr3+')|('+tmpRegStr4+')'\n",
    "        regStrs.append(regStr)\n",
    "    return regStrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 7.63 µs\n",
      "<class 'list'>\n",
      "Writing list to groupFeatureOriginPatternList...\n"
     ]
    }
   ],
   "source": [
    "writeToFileBasic(convertToPatternList(groupFeatureOrigin), 'groupFeatureOriginPatternList')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 7.63 µs\n"
     ]
    }
   ],
   "source": [
    "regStrs = convertToPatternList(groupFeatureOrigin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHitNumber(text):\n",
    "    \n",
    "    hit = 0\n",
    "    hitTweetNum = 0\n",
    "    tweetLengthNum = 0\n",
    "    tweetNum = 0\n",
    "    \n",
    "    \n",
    "    if type(text) == list:\n",
    "        text_list = text.split('\\n')  # 把一個user原本以\\n串連的發文分開\n",
    "        tweetNum = len(text_list)\n",
    "        for tweet in text_list:\n",
    "            hitFlag = False\n",
    "            tweet = \" \".join(tokenize(tweet))\n",
    "            for regStr in regStrs:\n",
    "                if re.search(re.compile(regStr), tweet) != None:\n",
    "                    hit += 1\n",
    "                    if hitFlag == False:\n",
    "                        hitTweetNum += 1\n",
    "                        hitFlag = True\n",
    "                    else:\n",
    "                        pass\n",
    "        return hit/hitTweetNum, \n",
    "    \n",
    "    elif type(text) == str: # 丟進來就是一段普通的字串\n",
    "        hit = 0\n",
    "        tweet = \" \".join(tokenize(text))\n",
    "        for regStr in regStrs:\n",
    "            if re.search(re.compile(regStr), tweet) != None:\n",
    "                hit += 1\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        print('Please stop putting weird stuff!')\n",
    "        return 'Not acceptable'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-253-e3ef975f3355>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgroupHitNum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mperson\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroup_texts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mgroupHitNum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetHitNumber\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperson\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-249-fcc96b5ac211>\u001b[0m in \u001b[0;36mgetHitNumber\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregStr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mregStrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregStr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m                 \u001b[0mhit\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/re.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \"\"\"Scan through string looking for a match to the pattern, returning\n\u001b[1;32m    172\u001b[0m     a match object, or None if no match was found.\"\"\"\n\u001b[0;32m--> 173\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "groupHitNum = []\n",
    "for person in group_texts:\n",
    "    groupHitNum.append(getHitNumber(person))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24, 501, 439, 193, 10, 390, 137, 46]"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupHitNum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
