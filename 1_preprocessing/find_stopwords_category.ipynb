{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import re, math\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from random import randint\n",
    "import ipynb.fs.defs.PeopleInfo as peopleInfo\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import TweetTokenizer\n",
    "from nltk import PorterStemmer\n",
    "# import ipynb.fs.defs.TweetTextHandler as tweetTextHandler\n",
    "import ipynb.fs.defs.FilterMethods as filterMethods\n",
    "import sys, os\n",
    "sys.path.append('../2_feature')\n",
    "import ipynb.fs.defs.GetFeatures as getFeatures\n",
    "global stop_words\n",
    "stop_words = stopwords.words('english')\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from IPython.display import display, Image\n",
    "plotly.tools.set_credentials_file(username='Adeline', api_key='Z5eltNtBQXqvI05ZFQtz')\n",
    "# import plotly.offline as offline\n",
    "# offline.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_by_symbols(txt):\n",
    "    txt = re.sub(r\"https\\S+\", '', txt)\n",
    "    txt = re.sub(r\"http\\S+\", '', txt)\n",
    "    txt = re.sub(r\"pic.twitter.com\\S+\", '', txt)\n",
    "    txt = re.sub(r\"twitter.com/\\S+\", '', txt)\n",
    "    txt = re.sub(r\"\\S+/\\S+\", '', txt)\n",
    "    txt = re.sub(r\"@\\S+\", '', txt)\n",
    "    txt = re.sub(r\"#\\S+\", '', txt)\n",
    "    txt = re.sub(r\"idk\", 'i do not know', txt)   # idk: i don't know\n",
    "    txt = re.sub(r\"tho\", 'though', txt)   # tho\n",
    "    txt = re.sub(r\"i\\'m\", 'i am', txt)\n",
    "    txt = re.sub(r\"you\\'re\", 'you are', txt)\n",
    "    txt = re.sub(r\"he\\'s\", 'he is', txt)\n",
    "    txt = re.sub(r\"she\\'s\", 'she is', txt)\n",
    "    txt = re.sub(r\"it\\'s\", 'it is', txt)\n",
    "    txt = re.sub(r\"we\\'re\", 'we are', txt)\n",
    "    txt = re.sub(r\"they\\'re\", 'they are', txt)\n",
    "    txt = re.sub(r\"isn\\'t\", 'is not', txt)\n",
    "    txt = re.sub(r\"don\\'t\", 'do not', txt)\n",
    "    txt = re.sub(r\"doesn\\'t\", 'does not', txt)\n",
    "    txt = re.sub(r\"didn\\'t\", 'did not', txt)\n",
    "    txt = re.sub(r\"wasn\\'t\", 'was not', txt)\n",
    "    txt = re.sub(r\"weren\\'t\", 'were not', txt)\n",
    "    txt = re.sub(r\"haven\\'t\", 'have not', txt)\n",
    "    txt = re.sub(r\"can\\'t\", 'can not', txt)\n",
    "    txt = re.sub(r\"couldn\\'t\", 'could not', txt)\n",
    "    txt = re.sub(r\"wouldn\\'t\", 'would not', txt)\n",
    "    txt = re.sub(r\"shouldn\\'t\", 'should not', txt)\n",
    "    txt = re.sub(r\"&amp\", '', txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text_list = re.findall('(?u)\\\\b[a-zA-Z]\\\\w{0,}\\\\b', text)\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create LIWC Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_category_dict = dict()\n",
    "liwc_word_dict = dict()\n",
    "with open('/home/adeline/Documents/Depression_Research/LIWC2007 Documents/Dictionaries/LIWC2007_English080730.dic') as open_file:\n",
    "    raw_data = open_file.readlines()\n",
    "    for row in raw_data[1:65]:\n",
    "        row = row.strip().split('\\t')\n",
    "        liwc_category_dict[row[0]] = row[1]\n",
    "    for row in raw_data[66:4553]:\n",
    "        row = row.strip().split('\\t')\n",
    "        liwc_word_dict[row[0]] = row[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\t['1', '2', '3', '4']\n",
      "me\t['1', '2', '3', '4']\n",
      "my\t['1', '2', '3', '4']\n",
      "myself\t['1', '2', '3', '4']\n",
      "we\t['1', '2', '3', '5', '121', '131', '138']\n",
      "our\t['1', '2', '3', '5', '121']\n",
      "ours\t['1', '2', '3', '5', '121']\n",
      "ourselves\t['1', '2', '3', '5', '121']\n",
      "you\t['1', '2', '3', '6', '121']\n",
      "your\t['1', '2', '3', '6', '121']\n",
      "yours\t['1', '2', '3', '6', '121']\n",
      "KeyError: yourself\n",
      "KeyError: yourselves\n",
      "he\t['1', '2', '3', '7', '121']\n",
      "him\t['1', '2', '3', '7', '121']\n",
      "his\t['1', '2', '3', '7', '121']\n",
      "himself\t['1', '2', '3', '7', '121']\n",
      "she\t['1', '2', '3', '7', '121']\n",
      "her\t['1', '2', '3', '7', '121']\n",
      "hers\t['1', '2', '3', '7', '121']\n",
      "herself\t['1', '2', '3', '7', '121']\n",
      "it\t['1', '2', '9']\n",
      "its\t['1', '2', '9']\n",
      "itself\t['1', '2', '9']\n",
      "they\t['1', '2', '3', '8', '121']\n",
      "them\t['1', '2', '3', '8', '121']\n",
      "KeyError: their\n",
      "KeyError: theirs\n",
      "themselves\t['1', '2', '3', '8', '121']\n",
      "what\t['1', '2', '9']\n",
      "which\t['1', '2', '9']\n",
      "who\t['1', '2', '9', '121']\n",
      "whom\t['1', '2', '9', '121']\n",
      "this\t['1', '2', '9']\n",
      "that\t['1', '2', '9']\n",
      "these\t['1', '2', '9']\n",
      "those\t['1', '2', '9']\n",
      "am\t['11', '1', '12', '14']\n",
      "is\t['11', '1', '12', '14']\n",
      "are\t['11', '1', '12', '14']\n",
      "was\t['11', '1', '12', '13']\n",
      "were\t['11', '1', '12', '13']\n",
      "be\t['11', '1', '12']\n",
      "been\t['11', '1', '12', '13']\n",
      "being\t['11', '1', '12']\n",
      "have\t['11', '1', '12', '14']\n",
      "has\t['11', '1', '12', '14']\n",
      "had\t['11', '1', '12', '13']\n",
      "having\t['11', '1', '12']\n",
      "do\t['11', '1', '12', '14']\n",
      "does\t['11', '1', '12', '14']\n",
      "did\t['11', '1', '12', '13']\n",
      "doing\t['11', '1', '12']\n",
      "a\t['1', '10']\n",
      "an\t['1', '10']\n",
      "the\t['1', '10']\n",
      "and\t['1', '18', '131', '138']\n",
      "but\t['1', '18', '131', '139']\n",
      "if\t['1', '18', '131', '134', '135', '139']\n",
      "or\t['1', '18', '131', '135', '139']\n",
      "because\t['1', '18', '131', '133']\n",
      "as\t['1', '17', '18']\n",
      "until\t['1', '17', '18', '253', '250']\n",
      "while\t['1', '18', '253', '250']\n",
      "of\t['1', '17']\n",
      "at\t['1', '17', '252', '250']\n",
      "by\t['1', '17']\n",
      "for\t['1', '17']\n",
      "with\t['1', '17', '131', '138']\n",
      "about\t['1', '16', '17']\n",
      "against\t['1', '17']\n",
      "between\t['1', '17']\n",
      "into\t['1', '17', '131', '138', '252', '250']\n",
      "KeyError: through\n",
      "during\t['1', '17', '253', '250']\n",
      "before\t['1', '17', '253', '250']\n",
      "after\t['1', '17', '253', '250']\n",
      "above\t['1', '17', '252', '250']\n",
      "below\t['1', '17', '252', '250']\n",
      "to\t['1', '17']\n",
      "from\t['1', '17']\n",
      "up\t['1', '17', '252', '250']\n",
      "down\t['1', '17', '252', '250']\n",
      "in\t['1', '17', '252', '250']\n",
      "out\t['1', '17', '131', '138', '252', '250']\n",
      "on\t['1', '17', '252', '250']\n",
      "off\t['1', '17', '252', '250']\n",
      "over\t['1', '17', '252', '250']\n",
      "under\t['1', '17', '252', '250']\n",
      "again\t['1', '16', '253', '250']\n",
      "KeyError: further\n",
      "then\t['1', '18', '253', '250']\n",
      "once\t['1', '21', '253', '250']\n",
      "here\t['1', '16']\n",
      "there\t['1', '16']\n",
      "when\t['1', '18', '253', '250', '16']\n",
      "where\t['1', '16', '252', '250']\n",
      "why\t['131', '133']\n",
      "how\t['1', '16', '18', '131', '133']\n",
      "all\t['1', '20', '131', '136']\n",
      "any\t['1', '20', '131', '135']\n",
      "both\t['1', '20', '131', '138', '252', '250']\n",
      "each\t['1', '20', '131', '138']\n",
      "KeyError: few\n",
      "more\t['1', '20']\n",
      "most\t['1', '20', '131', '135']\n",
      "other\t['1', '2', '9']\n",
      "some\t['1', '20', '131', '135']\n",
      "such\t['1', '16']\n",
      "no\t['1', '19']\n",
      "nor\t['1', '18', '19']\n",
      "not\t['1', '19', '131', '139']\n",
      "only\t['1', '16']\n",
      "own\t['1', '20']\n",
      "same\t['131', '1', '20']\n",
      "so\t['1', '16', '18']\n",
      "than\t['1', '17']\n",
      "too\t['1', '16']\n",
      "very\t['1', '16']\n",
      "KeyError: s\n",
      "KeyError: t\n",
      "can\t['11', '1', '12', '14']\n",
      "will\t['11', '1', '12', '15']\n",
      "just\t['1', '16', '131', '139']\n",
      "KeyError: don\n",
      "should\t['11', '1', '12', '15', '131', '134']\n",
      "now\t['1', '16', '253', '250']\n",
      "KeyError: d\n",
      "KeyError: ll\n",
      "KeyError: m\n",
      "KeyError: o\n",
      "KeyError: re\n",
      "KeyError: ve\n",
      "KeyError: y\n",
      "KeyError: ain\n",
      "KeyError: aren\n",
      "KeyError: couldn\n",
      "KeyError: didn\n",
      "KeyError: doesn\n",
      "KeyError: hadn\n",
      "KeyError: hasn\n",
      "KeyError: haven\n",
      "KeyError: isn\n",
      "ma\t['121', '122']\n",
      "KeyError: mightn\n",
      "KeyError: mustn\n",
      "KeyError: needn\n",
      "KeyError: shan\n",
      "KeyError: shouldn\n",
      "KeyError: wasn\n",
      "KeyError: weren\n",
      "won\t['11', '13', '125', '126', '355']\n",
      "KeyError: wouldn\n"
     ]
    }
   ],
   "source": [
    "for stop_word in stop_words:\n",
    "    try:\n",
    "        print('{0}\\t{1}'.format(stop_word, liwc_word_dict[stop_word]))\n",
    "    except KeyError:\n",
    "        print('KeyError: {0}'.format(stop_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patients = dict()\n",
    "ordinarys = dict()\n",
    "with open('../0_dataset/patient_ids') as r:\n",
    "    for patient in r.readlines()[:100]:\n",
    "        patient = patient.strip()\n",
    "        patients[patient] = peopleInfo.Patient(patient)\n",
    "with open('../0_dataset/ordinary_ids') as r:\n",
    "    for ordinary in r.readlines()[:100]:\n",
    "        ordinary = ordinary.strip()\n",
    "        ordinarys[ordinary] = peopleInfo.Ordinary(ordinary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove users:[]\n",
      "Remove users:[]\n"
     ]
    }
   ],
   "source": [
    "patients = filterMethods.filter_user_by_tweet_number(patients)\n",
    "ordinarys = filterMethods.filter_user_by_tweet_number(ordinarys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_texts = []\n",
    "group_texts = []\n",
    "\n",
    "for key in patients.keys():\n",
    "    group_texts.append(replace_by_symbols('\\n'.join(patients[key].getText())))\n",
    "\n",
    "for key in ordinarys.keys():\n",
    "    base_texts.append(replace_by_symbols('\\n'.join(ordinarys[key].getText())))\n",
    "\n",
    "corpus = base_texts + group_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Data Statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_filter(df):\n",
    "    # 過濾空列\n",
    "    filter = df['Text'] != ''\n",
    "    df = df[filter]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_tweets = []    # one element presented one tweet\n",
    "group_tweets = []\n",
    "for line in group_texts:\n",
    "    group_tweets.extend(line.split('\\n'))\n",
    "for line in base_texts:\n",
    "    base_tweets.extend(line.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet_group_len = [len(tokenize(x)) for x in group_tweets]\n",
    "# tweet_base_len = [len(tokenize(x)) for x in base_tweets]\n",
    "tweet_group_len = [len(x.split()) for x in group_tweets]\n",
    "tweet_base_len = [len(x.split()) for x in base_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfGroupTweets = df_filter(pd.DataFrame({'Text':group_tweets,'len':tweet_group_len}))\n",
    "dfBaseTweets = df_filter(pd.DataFrame({'Text':base_tweets,'len':tweet_base_len}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Brief Info of Group Tweets ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    99668.000000\n",
       "mean        11.514328\n",
       "std          8.246411\n",
       "min          0.000000\n",
       "25%          4.000000\n",
       "50%         10.000000\n",
       "75%         18.000000\n",
       "max         42.000000\n",
       "Name: len, dtype: float64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('=== Brief Info of Group Tweets ===')\n",
    "dfGroupTweets['len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Brief Info of Base Tweets ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    11908.000000\n",
       "mean        10.626386\n",
       "std          6.390020\n",
       "min          0.000000\n",
       "25%          5.000000\n",
       "50%          9.000000\n",
       "75%         15.000000\n",
       "max         33.000000\n",
       "Name: len, dtype: float64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('=== Brief Info of Base Tweets ===')\n",
    "dfBaseTweets['len'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_stopwrods_dict(text_list, mydict):\n",
    "\n",
    "    position = []\n",
    "    for i, text in enumerate(text_list):\n",
    "        if text in stop_words:    # 是個 stopword\n",
    "            position.append(i)\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    for i in range(len(position)):\n",
    "        try:\n",
    "            cur_pos = position[i]\n",
    "            next_pos = position[i+1]\n",
    "            key = '{0}-{1}-{2}'.format(text_list[cur_pos], text_list[next_pos], str(next_pos-cur_pos))\n",
    "            mydict[key] = mydict.get(key, 0) + 1\n",
    "        except IndexError:\n",
    "            continue\n",
    "    \n",
    "    \n",
    "    return mydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mydict = dict()\n",
    "for texts in group_texts:\n",
    "    for text in texts.split('\\n'):\n",
    "        construct_stopwrods_dict(tokenize(text), mydict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mydict2 = dict()\n",
    "for texts in base_texts:\n",
    "    for text in texts.split('\\n'):\n",
    "        construct_stopwrods_dict(tokenize(text), mydict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'mydict' (dict) to file 'stopwords_dict_depression'.\n"
     ]
    }
   ],
   "source": [
    "%store mydict >> stopwords_dict_depression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'mydict2' (dict) to file 'stopwords_dict_ordinary'.\n"
     ]
    }
   ],
   "source": [
    "%store mydict2 >> stopwords_dict_ordinary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
