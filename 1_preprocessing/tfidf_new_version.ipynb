{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import re, math\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from random import randint\n",
    "# import ipynb.fs.defs.PeopleInfo as peopleInfo\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import TweetTokenizer\n",
    "from nltk import PorterStemmer\n",
    "import ipynb.fs.defs.FilterMethods as filterMethods\n",
    "import sys, os\n",
    "sys.path.append('../2_feature')\n",
    "import ipynb.fs.defs.GetFeatures as getFeatures\n",
    "global stop_words\n",
    "stop_words = stopwords.words('english')\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from IPython.display import display, Image\n",
    "plotly.tools.set_credentials_file(username='Adeline', api_key='Z5eltNtBQXqvI05ZFQtz')\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "import datetime\n",
    "from matplotlib import dates\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "import matplotlib.dates as mdates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patient(object):\n",
    "    totalCount = 0\n",
    "    diagnosedTimeDict = dict()\n",
    "    idToNameDict = dict()\n",
    "    with open('../0_dataset/diagnosedTweetsPatch2') as open_file:\n",
    "        for line in open_file.readlines():\n",
    "            line = line.strip().split('\\t')\n",
    "            diagnosedTimeDict[line[0]] = datetime.datetime.strptime(line[3], \"%Y-%m-%d %H:%M:%S\")\n",
    "            idToNameDict[line[0]] = line[1]\n",
    "       \n",
    "    def __init__(self, userId):\n",
    "        self.userId = userId\n",
    "        self.name = Patient.idToNameDict[userId]\n",
    "        self.df0 = pd.read_csv('../0_dataset/DepressionUsersTweets/Patch2_data/'+userId,sep='\\t',header=None,usecols=[2,3],names=['Date','Text'],quoting=3,error_bad_lines=False,encoding='utf-8').dropna(axis=0, how='any')\n",
    "        self.df0 = self.df0.drop_duplicates(subset='Date', keep='first')\n",
    "        self.df0 = self.df0.set_index('Date')\n",
    "        self.df0.index = pd.to_datetime(self.df0.index)\n",
    "\n",
    "        self.diagnosedTime = Patient.diagnosedTimeDict[userId]\n",
    "        timeDuration1 = self.diagnosedTime - datetime.timedelta(days=90)\n",
    "        timeDuration2 = self.diagnosedTime - datetime.timedelta(days=180)\n",
    "        self.df1 = self.df0.loc[str(self.diagnosedTime):str(timeDuration1)]\n",
    "        self.df2 = self.df0.loc[str(timeDuration1):str(timeDuration2)]\n",
    "        Patient.totalCount += 1 \n",
    "        \n",
    "    def displayCount(self):\n",
    "        print(\"Total Patients {0}\".format(Patient.totalCount))\n",
    "    \n",
    "    def displayTweetsCount(self, category=1):\n",
    "        if category == 0:\n",
    "            return len(self.df0)\n",
    "        elif category == 1:\n",
    "            return len(self.df1)\n",
    "        elif category == 2:\n",
    "            return len(self.df2)\n",
    "        else:\n",
    "            print('Please indicate category (inRange or all)')\n",
    "            return False\n",
    "        \n",
    "    def getText(self, category=1):\n",
    "        if category == 0:\n",
    "            return self.df0['Text'].values\n",
    "        if category == 1:\n",
    "            return self.df1['Text'].values\n",
    "        elif category == 2:\n",
    "            return self.df2['Text'].values\n",
    "        else:\n",
    "            print('Please indicate category (inRange or all)')\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_by_symbols(txt):\n",
    "    txt = re.sub(r\"https\\S+\", '', txt)\n",
    "    txt = re.sub(r\"http\\S+\", '', txt)\n",
    "    txt = re.sub(r\"pic.twitter.com\\S+\", '', txt)\n",
    "    txt = re.sub(r\"twitter.com/\\S+\", '', txt)\n",
    "    txt = re.sub(r\"\\S+/\\S+\", '', txt)\n",
    "    txt = re.sub(r\"@\\S+\", '', txt)\n",
    "    txt = re.sub(r\"#\\S+\", '', txt)\n",
    "    txt = re.sub(r\"idk\", 'i do not know', txt)   # idk: i don't know\n",
    "    txt = re.sub(r\"tbh\", 'to be honest', txt)   # tbh: to be honest\n",
    "    txt = re.sub(r\"tho\", 'though', txt)   # tho\n",
    "    txt = re.sub(r\"i\\'m\", 'i am', txt)\n",
    "    txt = re.sub(r\"you\\'re\", 'you are', txt)\n",
    "    txt = re.sub(r\"he\\'s\", 'he is', txt)\n",
    "    txt = re.sub(r\"she\\'s\", 'she is', txt)\n",
    "    txt = re.sub(r\"it\\'s\", 'it is', txt)\n",
    "    txt = re.sub(r\"we\\'re\", 'we are', txt)\n",
    "    txt = re.sub(r\"they\\'re\", 'they are', txt)\n",
    "    txt = re.sub(r\"isn\\'t\", 'is not', txt)\n",
    "    txt = re.sub(r\"don\\'t\", 'do not', txt)\n",
    "    txt = re.sub(r\"doesn\\'t\", 'does not', txt)\n",
    "    txt = re.sub(r\"didn\\'t\", 'did not', txt)\n",
    "    txt = re.sub(r\"wasn\\'t\", 'was not', txt)\n",
    "    txt = re.sub(r\"weren\\'t\", 'were not', txt)\n",
    "    txt = re.sub(r\"haven\\'t\", 'have not', txt)\n",
    "    txt = re.sub(r\"can\\'t\", 'can not', txt)\n",
    "    txt = re.sub(r\"couldn\\'t\", 'could not', txt)\n",
    "    txt = re.sub(r\"wouldn\\'t\", 'would not', txt)\n",
    "    txt = re.sub(r\"shouldn\\'t\", 'should not', txt)\n",
    "    txt = re.sub(r\"&amp\", '', txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.6 s, sys: 52 ms, total: 1.66 s\n",
      "Wall time: 1.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "patients = dict()\n",
    "with open('../0_dataset/patch2PatientIDs') as openFile:\n",
    "    for patient in openFile.readlines():\n",
    "        patient = patient.strip()\n",
    "        patients[patient] = Patient(patient)\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# base_texts = []\n",
    "group_texts = []\n",
    "\n",
    "for key in patients.keys():\n",
    "    group_texts.append(replace_by_symbols('\\n'.join(patients[key].getText(1))))\n",
    "\n",
    "# for key in ordinarys.keys():\n",
    "#     base_texts.append(replace_by_symbols('\\n'.join(ordinarys[key].getText())))\n",
    "\n",
    "# corpus = base_texts + group_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorizer(ngrams=1):\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\", token_pattern='(?u)\\\\b[a-zA-Z]\\\\w{2,}\\\\b', ngram_range = (ngrams,ngrams), min_df=1)\n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_scores(vectorizer, tfidf_result):\n",
    "    # http://stackoverflow.com/questions/16078015/\n",
    "    scores = zip(vectorizer.get_feature_names(),\n",
    "                 np.asarray(tfidf_result.sum(axis=0)).ravel())\n",
    "    sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    words = []\n",
    "    scores = []\n",
    "    for item in sorted_scores:\n",
    "        words.append(item[0])\n",
    "        scores.append(item[1])\n",
    "    return words, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = create_vectorizer()\n",
    "tfidf_result = vectorizer.fit_transform(group_texts)\n",
    "words, scores = display_scores(vectorizer, tfidf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "just 15.5710522482\n",
      "like 14.9442976943\n",
      "love 11.5760331508\n",
      "know 8.14639317976\n",
      "good 6.87465270101\n",
      "want 6.70333379776\n",
      "people 6.50405708417\n",
      "really 6.33557510248\n",
      "shit 6.06214344154\n",
      "fuck 6.04191544798\n",
      "time 5.74418016352\n",
      "got 5.59225987869\n",
      "today 5.5766771962\n",
      "day 5.55936927208\n",
      "did 5.41035565804\n",
      "fucking 5.31691932914\n",
      "going 5.16847757787\n",
      "think 5.06596968172\n",
      "need 4.92763499858\n",
      "new 4.69978960361\n",
      "gonna 4.65245657381\n",
      "feel 4.50048453158\n",
      "lol 4.45591093829\n",
      "life 4.44754772597\n",
      "happy 4.38990850656\n",
      "make 3.94688577686\n",
      "lmao 3.90652723362\n",
      "thank 3.77173831567\n",
      "god 3.72633669\n",
      "video 3.68074494519\n",
      "does 3.64053041784\n",
      "wanna 3.50040100928\n",
      "follow 3.49097951501\n",
      "right 3.45836030879\n",
      "hate 3.43533657602\n",
      "better 3.20420883639\n",
      "man 3.05512789452\n",
      "best 3.01779739779\n",
      "okay 3.00758546574\n",
      "look 3.00602663577\n",
      "thing 2.99519795718\n",
      "yes 2.98078145333\n",
      "omg 2.97419351604\n",
      "friends 2.9119888107\n",
      "let 2.90469612947\n",
      "bad 2.90251133436\n",
      "hope 2.84351825722\n",
      "work 2.82317582552\n",
      "actually 2.8015453372\n",
      "say 2.79317040021\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    print(words[i],scores[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter out users (Case1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch2FilterOutCase1 = []\n",
    "with open('patch2FilterOutCase1') as open_file:\n",
    "    for line in open_file.readlines():\n",
    "        line = line.strip()\n",
    "        patch2FilterOutCase1.append(line)\n",
    "        try:\n",
    "            del patients[line]\n",
    "        except KeyError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "group_texts = []\n",
    "\n",
    "for key in patients.keys():\n",
    "    group_texts.append(replace_by_symbols('\\n'.join(patients[key].getText(1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = create_vectorizer()\n",
    "tfidf_result = vectorizer.fit_transform(group_texts)\n",
    "words, scores = display_scores(vectorizer, tfidf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "just 12.5693990997\n",
      "like 12.4306876504\n",
      "love 9.07527699758\n",
      "know 6.6528181685\n",
      "good 5.7216059736\n",
      "really 5.3027535116\n",
      "want 5.27326990107\n",
      "people 5.19184026681\n",
      "fuck 5.0079775563\n",
      "shit 4.99594358312\n",
      "time 4.79412937591\n",
      "day 4.63950289237\n",
      "got 4.53774766959\n",
      "did 4.35256194845\n",
      "fucking 4.32115320006\n",
      "today 4.27290431912\n",
      "think 4.12787881051\n",
      "going 4.04178678479\n",
      "gonna 3.88146080441\n",
      "need 3.83240002137\n",
      "new 3.78167009083\n",
      "lol 3.66843185208\n",
      "happy 3.58478404255\n",
      "feel 3.53966287542\n",
      "life 3.51191572816\n",
      "lmao 3.41314318803\n",
      "god 3.31839401953\n",
      "make 3.17128102804\n",
      "thank 2.97127675015\n",
      "wanna 2.90731860242\n",
      "right 2.73336646218\n",
      "hate 2.72662150802\n",
      "video 2.60991466044\n",
      "man 2.55823758137\n",
      "does 2.55172257944\n",
      "work 2.49138633262\n",
      "let 2.49134838183\n",
      "thing 2.43784951112\n",
      "better 2.43610531823\n",
      "look 2.43285245348\n",
      "best 2.41861538929\n",
      "omg 2.35168481564\n",
      "yes 2.33455000143\n",
      "yeah 2.3145499956\n",
      "actually 2.29906762588\n",
      "okay 2.29126109399\n",
      "say 2.28808025758\n",
      "hope 2.26109163709\n",
      "bad 2.23536299484\n",
      "night 2.20973465369\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    print(words[i],scores[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter (Case2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomSelectRows(userId, rowsPerDay=5):\n",
    "    \n",
    "    indices = []\n",
    "    values = []\n",
    "    df = patients[userId].df1\n",
    "    if patients[userId].displayTweetsCount(1) <= 450:\n",
    "        indices.extend(df.index)\n",
    "    else:\n",
    "        gp = df.index.groupby(df.index.day)\n",
    "\n",
    "        for key in gp.keys():\n",
    "            try:\n",
    "                rowIndex = np.random.choice(gp[key], rowsPerDay, replalce=False)\n",
    "            except:\n",
    "                rowIndex = np.random.choice(gp[key], rowsPerDay)\n",
    "            indices.extend(pd.to_datetime(rowIndex))\n",
    "        indices = list(set(indices))\n",
    "        i = len(indices)\n",
    "        while i < 450:\n",
    "            tmpIndices = pd.to_datetime(np.random.choice(df.index, 1))\n",
    "#             print('{0}\\t{1}'.format(i, tmpIndices))\n",
    "            if tmpIndices in indices:\n",
    "#                 print('{0} is already in indcies'.format(tmpIndices))\n",
    "                continue\n",
    "            else:\n",
    "                indices.extend(tmpIndices)\n",
    "                i += 1\n",
    "#     print(indices)\n",
    "    for index in indices:\n",
    "        if type(df.loc[index]['Text']) != str:\n",
    "            print(index, df.loc[index]['Text'])\n",
    "        values.append(df.loc[index]['Text'])\n",
    "        \n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesing 0\t2966191458...\n",
      "Procesing 1\t485255179...\n",
      "Procesing 2\t2468363603...\n",
      "Procesing 3\t720699847012192256...\n",
      "Procesing 4\t2246711085...\n",
      "Procesing 5\t4303354228...\n",
      "Procesing 6\t298843567...\n",
      "Procesing 7\t772661900...\n",
      "Procesing 8\t1626298430...\n",
      "Procesing 9\t2478963242...\n",
      "Procesing 10\t4057096000...\n",
      "Procesing 11\t570561813...\n",
      "Procesing 12\t874493282...\n",
      "Procesing 13\t1666467426...\n",
      "Procesing 14\t101959854...\n",
      "Procesing 15\t2851359600...\n",
      "Procesing 16\t562975438...\n",
      "Procesing 17\t3793973969...\n",
      "Procesing 18\t3474584555...\n",
      "Procesing 19\t315708868...\n",
      "Procesing 20\t169073155...\n",
      "Procesing 21\t2966022107...\n",
      "Procesing 22\t3289341230...\n",
      "Procesing 23\t804789932522872833...\n",
      "Procesing 24\t613658426...\n",
      "Procesing 25\t4411064547...\n",
      "Procesing 26\t756912267199995904...\n",
      "Procesing 27\t723170886015012864...\n",
      "Procesing 28\t2843195353...\n",
      "Procesing 29\t564143639...\n",
      "Procesing 30\t28265926...\n",
      "Procesing 31\t1417808906...\n",
      "Procesing 32\t36249905...\n",
      "Procesing 33\t2727420622...\n",
      "Procesing 34\t2793648772...\n",
      "Procesing 35\t2994318326...\n",
      "Procesing 36\t2428283527...\n",
      "Procesing 37\t4312293556...\n",
      "Procesing 38\t2556837663...\n",
      "Procesing 39\t1599613848...\n",
      "Procesing 40\t3097242490...\n",
      "Procesing 41\t1572035209...\n",
      "Procesing 42\t512568555...\n",
      "Procesing 43\t1689178962...\n",
      "Procesing 44\t2853936016...\n",
      "Procesing 45\t1556728736...\n",
      "Procesing 46\t3231951024...\n",
      "Procesing 47\t849047016755781632...\n",
      "Procesing 48\t2901348284...\n",
      "Procesing 49\t546167140...\n",
      "Procesing 50\t440125176...\n",
      "Procesing 51\t588755645...\n",
      "Procesing 52\t3314167624...\n",
      "Procesing 53\t2764962837...\n",
      "Procesing 54\t971196408...\n",
      "Procesing 55\t842129310794276869...\n",
      "Procesing 56\t552149630...\n",
      "Procesing 57\t258989894...\n",
      "Procesing 58\t64003205...\n",
      "Procesing 59\t244474995...\n",
      "Procesing 60\t3010786498...\n",
      "Procesing 61\t968537478...\n",
      "Procesing 62\t1242242515...\n",
      "Procesing 63\t2729692029...\n",
      "Procesing 64\t2719894570...\n",
      "Procesing 65\t63539936...\n",
      "Procesing 66\t3308949014...\n",
      "Procesing 67\t2570243838...\n",
      "Procesing 68\t760742554086375425...\n",
      "Procesing 69\t708989209...\n",
      "Procesing 70\t410666385...\n",
      "Procesing 71\t849811970...\n",
      "Procesing 72\t4408416261...\n",
      "Procesing 73\t2956731116...\n",
      "Procesing 74\t321625598...\n",
      "Procesing 75\t2417781601...\n",
      "Procesing 76\t465631083...\n",
      "Procesing 77\t2230513924...\n",
      "Procesing 78\t3381321028...\n",
      "Procesing 79\t848009056468049920...\n",
      "Procesing 80\t97544338...\n",
      "Procesing 81\t1134890323...\n",
      "Procesing 82\t509506594...\n",
      "Procesing 83\t601814126...\n",
      "Procesing 84\t4892904785...\n",
      "Procesing 85\t717906721595793408...\n",
      "Procesing 86\t41449588...\n",
      "Procesing 87\t1896020964...\n",
      "Procesing 88\t1689505880...\n",
      "Procesing 89\t2504152388...\n",
      "Procesing 90\t563154554...\n",
      "Procesing 91\t3707770516...\n",
      "Procesing 92\t822988837127331840...\n",
      "Procesing 93\t3038200924...\n",
      "Procesing 94\t2525940708...\n",
      "Procesing 95\t3001880453...\n",
      "Procesing 96\t349888967...\n",
      "Procesing 97\t1665435030...\n",
      "Procesing 98\t3228273434...\n",
      "Procesing 99\t2946958093...\n",
      "Procesing 100\t565671290...\n",
      "Procesing 101\t617867879...\n",
      "Procesing 102\t19038462...\n",
      "Procesing 103\t4772101398...\n",
      "Procesing 104\t222204532...\n",
      "Procesing 105\t1129117406...\n",
      "Procesing 106\t2942676093...\n",
      "Procesing 107\t337385643...\n",
      "Procesing 108\t417059302...\n",
      "Procesing 109\t833626585...\n",
      "Procesing 110\t2683056127...\n",
      "Procesing 111\t721845845574885376...\n",
      "Procesing 112\t563002406...\n",
      "Procesing 113\t186779133...\n",
      "Procesing 114\t67243205...\n",
      "Procesing 115\t3019223763...\n",
      "Procesing 116\t2882647911...\n",
      "Procesing 117\t2387308772...\n",
      "Procesing 118\t419745295...\n",
      "Procesing 119\t425417854...\n",
      "Procesing 120\t2598896815...\n",
      "Procesing 121\t1311984584...\n",
      "Procesing 122\t1084422662...\n",
      "Procesing 123\t2951422317...\n",
      "Procesing 124\t462417957...\n",
      "Procesing 125\t246098522...\n",
      "Procesing 126\t2693290879...\n",
      "Procesing 127\t125700525...\n",
      "Procesing 128\t109301470...\n",
      "Procesing 129\t1841952644...\n",
      "Procesing 130\t142145802...\n",
      "Procesing 131\t2911250971...\n",
      "Procesing 132\t2608861754...\n",
      "Procesing 133\t3078411003...\n",
      "Procesing 134\t240460467...\n",
      "Procesing 135\t284285972...\n",
      "Procesing 136\t1263469376...\n"
     ]
    }
   ],
   "source": [
    "# 每人每天5篇，總共450篇（不滿就全取）\n",
    "%%time\n",
    "group_texts = []\n",
    "print('Total: {0}'.format(len(patients)))\n",
    "for i, patient in enumerate(patients):\n",
    "    print('Procesing {0}\\t{1}...'.format(i, patient))\n",
    "    values = randomSelectRows(patient, 5)\n",
    "    text = replace_by_symbols('\\n'.join(values))\n",
    "    group_texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "just 13.251386251\n",
      "like 12.6759625785\n",
      "love 10.1064637363\n",
      "know 7.05588078773\n",
      "good 5.71563453578\n",
      "want 5.68239696985\n",
      "people 5.62909106717\n",
      "fuck 5.31558053484\n",
      "shit 5.20941546475\n",
      "today 5.13584579144\n",
      "really 5.12161574733\n",
      "got 4.97468696209\n",
      "time 4.88386201809\n",
      "day 4.77172255352\n",
      "did 4.70543072846\n",
      "going 4.52556028881\n",
      "think 4.45464595889\n",
      "fucking 4.42159899843\n",
      "need 4.20096915967\n",
      "new 4.09025085855\n",
      "gonna 3.98456210386\n",
      "lol 3.90347251054\n",
      "feel 3.80748608081\n",
      "happy 3.76655322713\n",
      "life 3.73950027815\n",
      "video 3.48662167088\n",
      "make 3.39587988818\n",
      "follow 3.31755675488\n",
      "wanna 3.2003063673\n",
      "does 3.12400829222\n",
      "hate 3.08508546441\n",
      "right 3.0575187109\n",
      "man 2.97186397144\n",
      "thank 2.92853574343\n",
      "god 2.90751134198\n",
      "lmao 2.8268708074\n",
      "better 2.73961009067\n",
      "best 2.68009303153\n",
      "work 2.6079102691\n",
      "friends 2.57349168896\n",
      "okay 2.5431811901\n",
      "thing 2.51064183145\n",
      "ass 2.50489161177\n",
      "bad 2.47795693509\n",
      "hope 2.47577004242\n",
      "school 2.47393609831\n",
      "depression 2.44831675986\n",
      "say 2.43705729885\n",
      "yeah 2.42367599246\n",
      "look 2.4174312008\n"
     ]
    }
   ],
   "source": [
    "vectorizer = create_vectorizer()\n",
    "tfidf_result = vectorizer.fit_transform(group_texts)\n",
    "words, scores = display_scores(vectorizer, tfidf_result)\n",
    "for i in range(50):\n",
    "    print(words[i],scores[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
