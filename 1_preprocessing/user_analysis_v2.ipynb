{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import ipynb.fs.defs.PeopleInfo as peopleInfo\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import TweetTokenizer\n",
    "# import ipynb.fs.defs.TweetTextHandler as tweetTextHandler\n",
    "import ipynb.fs.defs.FilterMethods as filterMethods\n",
    "import sys, os\n",
    "sys.path.append('../2_feature')\n",
    "import ipynb.fs.defs.GetFeatures as getFeatures\n",
    "global stop_words\n",
    "stop_words = stopwords.words('english')\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from IPython.display import display, Image\n",
    "plotly.tools.set_credentials_file(username='Adeline', api_key='Z5eltNtBQXqvI05ZFQtz')\n",
    "# import plotly.offline as offline\n",
    "# offline.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getWordsDict(texts):\n",
    "    global stop_words\n",
    "    wordDict = dict()\n",
    "    stopwordDict = dict()\n",
    "    stopwordTweetCount = 0\n",
    "    tweet_length = 0\n",
    "    tknzr = TweetTokenizer()\n",
    "    aaa = texts\n",
    "    length = len(texts)\n",
    "    for txt in texts:\n",
    "        stopwordFlag = False\n",
    "        try:\n",
    "            txt = filterMethods.filter_tweet_by_re(txt)\n",
    "            tweet_length += len(txt)\n",
    "            for token in tknzr.tokenize(txt):\n",
    "                if token.isdigit():\n",
    "                    pass\n",
    "                elif token in stop_words:\n",
    "                    stopwordDict[token] = stopwordDict.get(token, 0) + 1\n",
    "                    stopwordFlag = True\n",
    "                else:\n",
    "                    wordDict[token] = wordDict.get(token, 0) + 1\n",
    "            if stopwordFlag:\n",
    "                stopwordTweetCount += 1\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    return wordDict, stopwordDict, stopwordTweetCount, round(tweet_length/length,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patients = dict()\n",
    "ordinarys = dict()\n",
    "with open('../0_dataset/patient_ids') as r:\n",
    "    for patient in r.readlines()[:100]:\n",
    "        patient = patient.strip()\n",
    "        patients[patient] = peopleInfo.Patient(patient)\n",
    "with open('../0_dataset/ordinary_ids') as r:\n",
    "    for ordinary in r.readlines()[:100]:\n",
    "        ordinary = ordinary.strip()\n",
    "        ordinarys[ordinary] = peopleInfo.Ordinary(ordinary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove users:['3031516391', '2994889032']\n",
      "Remove users:[]\n"
     ]
    }
   ],
   "source": [
    "patients = filterMethods.filter_user_by_tweet_number(patients)\n",
    "ordinarys = filterMethods.filter_user_by_tweet_number(ordinarys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Stopwords between Ordinary People and Patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "patients_total = [0, 0, 0, 0]\n",
    "for key in patients:\n",
    "    _w, _s, _sc, _tl = getWordsDict(patients[key].getText())\n",
    "    patients_total[0] += sum(_s.values())\n",
    "    patients_total[1] += _sc\n",
    "    patients_total[2] += len(patients[key].inRangeDf)\n",
    "    patients_total[3] += _tl\n",
    "    \n",
    "patients_total[3] = round(patients_total[3]/len(patients),1)\n",
    "\n",
    "# 0:Stopwords總數/ 1:含有stopwords的發文總數/ 2:（時間內的）發文總數/ 3:平均發文長度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ordinarys_total = [0, 0, 0, 0]\n",
    "for key in ordinarys:\n",
    "    _w, _s, _sc, _tl = getWordsDict(ordinarys[key].getText())\n",
    "    ordinarys_total[0] += sum(_s.values())\n",
    "    ordinarys_total[1] += _sc\n",
    "    ordinarys_total[2] += len(ordinarys[key].inRangeDf)\n",
    "    ordinarys_total[3] += _tl\n",
    "    \n",
    "ordinarys_total[3] = round(ordinarys_total[3]/len(ordinarys),1)\n",
    "# 0:Stopwords總數/ 1:含有stopwords的推文總數/ 2:（時間內的）推文總數/ 3:平均發文長度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the format of your plot grid:\n",
      "[ (1,1) x1,y1 ]  [ (1,2) x2,y2 ]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Adeline/194.embed\" height=\"600px\" width=\"900px\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace1_p = go.Bar(x=['stopwords佔每篇發文的數量', '有stopwords的發文佔總發文數量'],\n",
    "                  y=[round(patients_total[0]/patients_total[1],1), round(patients_total[1]/patients_total[2],1)],\n",
    "                  name='Patients',\n",
    "                  marker=dict(color='#ffcdd2'))\n",
    "\n",
    "trace1_o = go.Bar(x=['stopwords佔每篇發文的數量', '有stopwords的發文佔總發文數量'],\n",
    "                y=[round(ordinarys_total[0]/ordinarys_total[1],1), round(ordinarys_total[1]/ordinarys_total[2],1)],\n",
    "                name='Ordinarys',\n",
    "                marker=dict(color='#A2D5F2'))\n",
    "\n",
    "trace2_p = go.Bar(x=['發文長度'],\n",
    "                y=[patients_total[3]],\n",
    "                marker=dict(color='#ffcdd2'))\n",
    "\n",
    "trace2_o = go.Bar(x=['發文長度'],\n",
    "                y=[ordinarys_total[3]],\n",
    "                marker=dict(color='#A2D5F2'))\n",
    "\n",
    "fig = plotly.tools.make_subplots(rows=1, cols=2)\n",
    "fig.append_trace(trace1_p,1,1)\n",
    "fig.append_trace(trace1_o,1,1)\n",
    "fig.append_trace(trace2_p,1,2)\n",
    "fig.append_trace(trace2_o,1,2)\n",
    "fig['layout'].update(height=600, width=900, title='Stopwords 比較')\n",
    "# fig3 = go.Figure(data=data, layout=layout)\n",
    "py.image.save_as(fig, filename='img/stopwords_compare.png')\n",
    "# display(Image(filename='img/stopwords_compare.png'))\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_texts = []\n",
    "group_texts = []\n",
    "\n",
    "for key in ordinarys.keys():\n",
    "    base_texts.append(filter_tweet('\\n'.join(ordinarys[key].getText())))\n",
    "\n",
    "for key in patients.keys():\n",
    "    group_texts.append(filter_tweet('\\n'.join(patients[key].getText())))\n",
    "\n",
    "corpus = base_texts + group_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=128, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=\"english\",ngram_range = (1,2), min_df=1)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "idf = vectorizer.idf_\n",
    "Y = np.array([0] * len(base_texts) + [1]*len(group_texts), dtype=int)\n",
    "classifier  = RandomForestClassifier(n_jobs=-1, max_features=\"sqrt\", n_estimators=128)\n",
    "classifier.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_indicies = np.argsort(classifier.feature_importances_)\n",
    "vector2word = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = []\n",
    "for i in range(50):\n",
    "    vector = feature_indicies[-i-1]\n",
    "    word = vector2word[vector]\n",
    "    top_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['diagnosed',\n",
       " 'pic',\n",
       " 'people',\n",
       " 'depression',\n",
       " 'scared',\n",
       " 'fact',\n",
       " 'right',\n",
       " 'feeling',\n",
       " 'love',\n",
       " 'pic love',\n",
       " 'throw',\n",
       " 'came',\n",
       " 'ass',\n",
       " 'fucked',\n",
       " 'problems',\n",
       " 'confused',\n",
       " 'stay',\n",
       " 'pure',\n",
       " 'make sure',\n",
       " 'energy',\n",
       " 'tho',\n",
       " 'shit',\n",
       " 'stop',\n",
       " 'door',\n",
       " 'hot',\n",
       " 'sorry',\n",
       " 'shitty',\n",
       " 'ice',\n",
       " 'lost',\n",
       " 'thank',\n",
       " 'watched',\n",
       " 'honestly',\n",
       " 'took',\n",
       " 'll',\n",
       " 'like just',\n",
       " '13',\n",
       " 'words',\n",
       " 'told',\n",
       " 'protect',\n",
       " 'boobs',\n",
       " 'dude',\n",
       " 'real',\n",
       " 'currently',\n",
       " 'mental',\n",
       " 'wtf',\n",
       " 'surprised',\n",
       " 'laugh',\n",
       " 'hours',\n",
       " 'wasn',\n",
       " 'bit']"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning:\n",
      "\n",
      "This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(Y, 10, random_state=randint(0,65536))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 0 0 1 0 1 0\n",
      " 0 0 0 0 1 1 1 0 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1\n",
      " 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 1 1 0 0 0 0 1 1 0 1 1 0 1 1 0 0 0 1 1 1 1 0\n",
      " 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 1 1 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 0 0 1 1 0\n",
      " 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "precisions = []\n",
    "for train_index, test_index in sss:\n",
    "    \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "    classifier = classifier.fit(X_train, Y_train)\n",
    "    score = classifier.score(X_test, Y_test)\n",
    "    precisions.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67000000000000004"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=128, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer2 = TfidfVectorizer(stop_words=\"english\",ngram_range = (1,2), min_df=1)\n",
    "X2 = vectorizer2.fit_transform(group_texts+group_texts)\n",
    "idf2 = vectorizer2.idf_\n",
    "Y2 = np.array([0] * len(group_texts) + [1]*len(group_texts), dtype=int)\n",
    "classifier2  = RandomForestClassifier(n_jobs=-1, max_features=\"sqrt\", n_estimators=128)\n",
    "classifier2.fit(X2,Y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_indicies2 = np.argsort(classifier2.feature_importances_)\n",
    "vector2word2 = vectorizer2.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_words2 = []\n",
    "for i in range(50):\n",
    "    vector2 = feature_indicies2[-i-1]\n",
    "    word2 = vector2word2[vector2]\n",
    "    top_words2.append(word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['caught',\n",
       " 'second',\n",
       " 'feel bad',\n",
       " 'af',\n",
       " 'does',\n",
       " 'thanks',\n",
       " 'feel',\n",
       " 'really good',\n",
       " 'friends',\n",
       " 'miss',\n",
       " 'lied',\n",
       " 'send',\n",
       " 'new',\n",
       " 'precious',\n",
       " 'feels',\n",
       " 'fellow',\n",
       " 'year old',\n",
       " 'truth',\n",
       " 'born',\n",
       " 'walk away',\n",
       " 'hi',\n",
       " 'sure',\n",
       " 'stop',\n",
       " 'song',\n",
       " 'wonderful',\n",
       " 'boo',\n",
       " 'pants',\n",
       " 'lover',\n",
       " 'strike',\n",
       " 'candidate',\n",
       " 'right',\n",
       " 'held',\n",
       " 'sauce',\n",
       " 'france',\n",
       " 'gonna',\n",
       " 'weight',\n",
       " 'graduation',\n",
       " 'depression',\n",
       " 'away',\n",
       " 'relate',\n",
       " 'long',\n",
       " 'love ve',\n",
       " 'life got',\n",
       " 'distract',\n",
       " 'pic',\n",
       " '30am',\n",
       " 'cooking',\n",
       " 'thought thing',\n",
       " 'register',\n",
       " 'need']"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(Y2, 1, random_state=randint(0,65536))\n",
    "precisions2 = []\n",
    "for train_index, test_index in sss:\n",
    "    \n",
    "    X_train, X_test = X2[train_index], X2[test_index]\n",
    "    Y_train, Y_test = Y2[train_index], Y2[test_index]\n",
    "    classifier = classifier2.fit(X_train, Y_train)\n",
    "    score = classifier2.score(X_test, Y_test)\n",
    "    precisions2.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.052631578947368418"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(precisions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
