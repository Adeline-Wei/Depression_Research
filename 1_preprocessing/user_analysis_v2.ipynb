{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import ipynb.fs.defs.PeopleInfo as peopleInfo\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import TweetTokenizer\n",
    "# from nltk.stem.porter import PorterStemmer\n",
    "# 引用 tweetTextHandler.ipynb\n",
    "import ipynb.fs.defs.TweetTextHandler as tweetTextHandler\n",
    "import ipynb.fs.defs.FilterMethods as filterMethods\n",
    "import sys, os\n",
    "sys.path.append('../2_feature')\n",
    "import ipynb.fs.defs.GetFeatures as getFeatures\n",
    "global stop_words\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getWordsDict(texts):\n",
    "    global stop_words\n",
    "    wordDict = dict()\n",
    "    stopwordDict = dict()\n",
    "    stopwordTweetCount = 0\n",
    "    tknzr = TweetTokenizer()\n",
    "\n",
    "    for txt in texts:\n",
    "        stopwordFlag = False\n",
    "        try:\n",
    "            txt = tweetTextHandler.del_url(txt)\n",
    "            for token in tknzr.tokenize(txt):\n",
    "                #token = porter.stem(token.lower())\n",
    "                if token.isdigit():\n",
    "                    pass\n",
    "                elif token in stop_words:\n",
    "                    stopwordDict[token] = stopwordDict.get(token, 0) + 1\n",
    "                    stopwordFlag = True\n",
    "                else:\n",
    "                    wordDict[token] = wordDict.get(token, 0) + 1\n",
    "            if stopwordFlag:\n",
    "                stopwordTweetCount += 1\n",
    "        except Exception as e:\n",
    "            #print(\"Error message:\", e)\n",
    "            pass\n",
    "    return wordDict, stopwordDict, stopwordTweetCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patients = dict()\n",
    "ordinarys = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('../0_dataset/patient_ids') as r:\n",
    "    for patient in r.readlines()[:100]:\n",
    "        patient = patient.strip()\n",
    "        patients[patient] = peopleInfo.Patient(patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove users:['181095972', '2409775369', '363595479', '2382824556', '113666708', '2195175613', '402683346', '3031516391', '1229103648']\n"
     ]
    }
   ],
   "source": [
    "patients = filterMethods.by_tweets_number(patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('../0_dataset/ordinary_ids') as r:\n",
    "    for ordinary in r.readlines()[:100]:\n",
    "        ordinary = ordinary.strip()\n",
    "        ordinarys[ordinary] = peopleInfo.Ordinary(ordinary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove users:[]\n"
     ]
    }
   ],
   "source": [
    "ordinarys = filterMethods.by_tweets_number(ordinarys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import plotly\n",
    "# import plotly.offline as offline\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "# from IPython.display import display\n",
    "plotly.tools.set_credentials_file(username='Adeline', api_key='Z5eltNtBQXqvI05ZFQtz')\n",
    "# offline.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total3 = [0, 0, 0]\n",
    "for key in patients:\n",
    "    _w, _s, _sc = getWordsDict(patients[key].getText())\n",
    "    total3[0] += sum(_s.values())\n",
    "    total3[1] += _sc\n",
    "    total3[2] += len(patients[key].inRangeDf)\n",
    "    \n",
    "# 0:Stopwords總數/ 1:含有stopwords的推文總數/ 2:（時間內的）推文總數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total4 = [0, 0, 0]\n",
    "for key in ordinarys:\n",
    "    _w, _s, _sc = getWordsDict(ordinarys[key].getText())\n",
    "    total4[0] += sum(_s.values())\n",
    "    total4[1] += _sc\n",
    "    total4[2] += len(ordinarys[key].inRangeDf)\n",
    "\n",
    "# 0:Stopwords總數/ 1:含有stopwords的推文總數/ 2:（時間內的）推文總數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trace_patients = go.Bar(x=['Type1', 'Type2'],\n",
    "                  y=[round(total3[0]/total3[1],1), round(total3[1]/total3[2],1)],\n",
    "                  name='Patients',\n",
    "                  marker=dict(color='#ffcdd2'))\n",
    "\n",
    "trace_ordinarys = go.Bar(x=['Type1', 'Type2'],\n",
    "                y=[round(total4[0]/total4[1],1), round(total4[1]/total4[2],1)],\n",
    "                name='Ordinarys',\n",
    "                marker=dict(color='#A2D5F2'))\n",
    "data = [trace_patients, trace_ordinarys]\n",
    "\n",
    "layout = go.Layout(title='比較',\n",
    "                xaxis=dict(title='使用者類型'),\n",
    "                yaxis=dict(title='平均數量'))\n",
    "\n",
    "fig3 = go.Figure(data=data, layout=layout)\n",
    "py.image.save_as(fig3, filename='img/a-simple-plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Adeline/184.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py.iplot(fig3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tweet(txt):\n",
    "    \n",
    "    try:\n",
    "        # url\n",
    "        txt = re.sub(r\"http\\S+\", \"\", txt)\n",
    "        txt = re.sub(r\"https\\S+\", \"\", txt)\n",
    "        txt = re.sub(r\"twitter.com/\\S+\", \"\", txt)\n",
    "        txt = re.sub(r\"pic.twitter.com\\S+\", \"\", txt)\n",
    "        txt = re.sub(r\"\\S+/\\S+\",\"\", txt)\n",
    "        #txt = re.sub(r\"\\S+\\/status\\S+\", txt)\n",
    "        # mention\n",
    "        txt = re.sub(r\"@\\S+\", \"\", txt)\n",
    "    except TypeError as e:\n",
    "        print(e)\n",
    "    \n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_texts = []\n",
    "group_texts = []\n",
    "\n",
    "for key in ordinarys.keys():\n",
    "    base_texts.append(filter_tweet('\\n'.join(ordinarys[key].getText())))\n",
    "\n",
    "for key in patients.keys():\n",
    "    group_texts.append(filter_tweet('\\n'.join(patients[key].getText())))\n",
    "\n",
    "corpus = base_texts + group_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=128, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=\"english\",ngram_range = (1,2), min_df=1)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "idf = vectorizer.idf_\n",
    "Y = np.array([0] * len(base_texts) + [1]*len(group_texts), dtype=int)\n",
    "classifier  = RandomForestClassifier(n_jobs=-1, max_features=\"sqrt\", n_estimators=128)\n",
    "classifier.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_indicies = np.argsort(classifier.feature_importances_)\n",
    "vector2word = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = []\n",
    "for i in range(50):\n",
    "    vector = feature_indicies[-i-1]\n",
    "    word = vector2word[vector]\n",
    "    top_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['diagnosed',\n",
       " 'pic',\n",
       " 'people',\n",
       " 'depression',\n",
       " 'scared',\n",
       " 'fact',\n",
       " 'right',\n",
       " 'feeling',\n",
       " 'love',\n",
       " 'pic love',\n",
       " 'throw',\n",
       " 'came',\n",
       " 'ass',\n",
       " 'fucked',\n",
       " 'problems',\n",
       " 'confused',\n",
       " 'stay',\n",
       " 'pure',\n",
       " 'make sure',\n",
       " 'energy',\n",
       " 'tho',\n",
       " 'shit',\n",
       " 'stop',\n",
       " 'door',\n",
       " 'hot',\n",
       " 'sorry',\n",
       " 'shitty',\n",
       " 'ice',\n",
       " 'lost',\n",
       " 'thank',\n",
       " 'watched',\n",
       " 'honestly',\n",
       " 'took',\n",
       " 'll',\n",
       " 'like just',\n",
       " '13',\n",
       " 'words',\n",
       " 'told',\n",
       " 'protect',\n",
       " 'boobs',\n",
       " 'dude',\n",
       " 'real',\n",
       " 'currently',\n",
       " 'mental',\n",
       " 'wtf',\n",
       " 'surprised',\n",
       " 'laugh',\n",
       " 'hours',\n",
       " 'wasn',\n",
       " 'bit']"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning:\n",
      "\n",
      "This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(Y, 10, random_state=randint(0,65536))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 0 0 1 0 1 0\n",
      " 0 0 0 0 1 1 1 0 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1\n",
      " 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 1 1 0 0 0 0 1 1 0 1 1 0 1 1 0 0 0 1 1 1 1 0\n",
      " 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 1 1 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 0 0 1 1 0\n",
      " 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "precisions = []\n",
    "for train_index, test_index in sss:\n",
    "    \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "    classifier = classifier.fit(X_train, Y_train)\n",
    "    score = classifier.score(X_test, Y_test)\n",
    "    precisions.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67000000000000004"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=128, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer2 = TfidfVectorizer(stop_words=\"english\",ngram_range = (1,2), min_df=1)\n",
    "X2 = vectorizer2.fit_transform(group_texts+group_texts)\n",
    "idf2 = vectorizer2.idf_\n",
    "Y2 = np.array([0] * len(group_texts) + [1]*len(group_texts), dtype=int)\n",
    "classifier2  = RandomForestClassifier(n_jobs=-1, max_features=\"sqrt\", n_estimators=128)\n",
    "classifier2.fit(X2,Y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_indicies2 = np.argsort(classifier2.feature_importances_)\n",
    "vector2word2 = vectorizer2.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_words2 = []\n",
    "for i in range(50):\n",
    "    vector2 = feature_indicies2[-i-1]\n",
    "    word2 = vector2word2[vector2]\n",
    "    top_words2.append(word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['caught',\n",
       " 'second',\n",
       " 'feel bad',\n",
       " 'af',\n",
       " 'does',\n",
       " 'thanks',\n",
       " 'feel',\n",
       " 'really good',\n",
       " 'friends',\n",
       " 'miss',\n",
       " 'lied',\n",
       " 'send',\n",
       " 'new',\n",
       " 'precious',\n",
       " 'feels',\n",
       " 'fellow',\n",
       " 'year old',\n",
       " 'truth',\n",
       " 'born',\n",
       " 'walk away',\n",
       " 'hi',\n",
       " 'sure',\n",
       " 'stop',\n",
       " 'song',\n",
       " 'wonderful',\n",
       " 'boo',\n",
       " 'pants',\n",
       " 'lover',\n",
       " 'strike',\n",
       " 'candidate',\n",
       " 'right',\n",
       " 'held',\n",
       " 'sauce',\n",
       " 'france',\n",
       " 'gonna',\n",
       " 'weight',\n",
       " 'graduation',\n",
       " 'depression',\n",
       " 'away',\n",
       " 'relate',\n",
       " 'long',\n",
       " 'love ve',\n",
       " 'life got',\n",
       " 'distract',\n",
       " 'pic',\n",
       " '30am',\n",
       " 'cooking',\n",
       " 'thought thing',\n",
       " 'register',\n",
       " 'need']"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(Y2, 1, random_state=randint(0,65536))\n",
    "precisions2 = []\n",
    "for train_index, test_index in sss:\n",
    "    \n",
    "    X_train, X_test = X2[train_index], X2[test_index]\n",
    "    Y_train, Y_test = Y2[train_index], Y2[test_index]\n",
    "    classifier = classifier2.fit(X_train, Y_train)\n",
    "    score = classifier2.score(X_test, Y_test)\n",
    "    precisions2.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.052631578947368418"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(precisions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
