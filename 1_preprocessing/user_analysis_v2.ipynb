{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import ipynb.fs.defs.PeopleInfo as peopleInfo\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import TweetTokenizer\n",
    "# from nltk.stem.porter import PorterStemmer\n",
    "# 引用 tweetTextHandler.ipynb\n",
    "import ipynb.fs.defs.TweetTextHandler as tweetTextHandler\n",
    "import ipynb.fs.defs.FilterMethods as filterMethods\n",
    "import sys, os\n",
    "sys.path.append('../2_feature')\n",
    "import ipynb.fs.defs.GetFeatures as getFeatures\n",
    "global stop_words\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getWordsDict(texts):\n",
    "    global stop_words\n",
    "    wordDict = dict()\n",
    "    stopwordDict = dict()\n",
    "    stopwordTweetCount = 0\n",
    "    tknzr = TweetTokenizer()\n",
    "\n",
    "    for txt in texts:\n",
    "        stopwordFlag = False\n",
    "        try:\n",
    "            txt = tweetTextHandler.del_url(txt)\n",
    "            for token in tknzr.tokenize(txt):\n",
    "                #token = porter.stem(token.lower())\n",
    "                if token.isdigit():\n",
    "                    pass\n",
    "                elif token in stop_words:\n",
    "                    stopwordDict[token] = stopwordDict.get(token, 0) + 1\n",
    "                    stopwordFlag = True\n",
    "                else:\n",
    "                    wordDict[token] = wordDict.get(token, 0) + 1\n",
    "            if stopwordFlag:\n",
    "                stopwordTweetCount += 1\n",
    "        except Exception as e:\n",
    "            #print(\"Error message:\", e)\n",
    "            pass\n",
    "    return wordDict, stopwordDict, stopwordTweetCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patients = dict()\n",
    "ordinarys = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('../0_dataset/patient_ids') as r:\n",
    "    for patient in r.readlines()[:100]:\n",
    "        patient = patient.strip()\n",
    "        patients[patient] = peopleInfo.Patient(patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove users:['181095972', '2409775369', '363595479', '2382824556', '113666708', '2195175613', '402683346', '3031516391', '1229103648']\n"
     ]
    }
   ],
   "source": [
    "patients = filterMethods.by_tweets_number(patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('../0_dataset/ordinary_ids') as r:\n",
    "    for ordinary in r.readlines()[:100]:\n",
    "        ordinary = ordinary.strip()\n",
    "        ordinarys[ordinary] = peopleInfo.Ordinary(ordinary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove users:[]\n"
     ]
    }
   ],
   "source": [
    "ordinarys = filterMethods.by_tweets_number(ordinarys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import plotly\n",
    "# import plotly.offline as offline\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "# from IPython.display import display\n",
    "plotly.tools.set_credentials_file(username='Adeline', api_key='Z5eltNtBQXqvI05ZFQtz')\n",
    "# offline.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total3 = [0, 0, 0]\n",
    "for key in patients:\n",
    "    _w, _s, _sc = getWordsDict(patients[key].getText())\n",
    "    total3[0] += sum(_s.values())\n",
    "    total3[1] += _sc\n",
    "    total3[2] += len(patients[key].inRangeDf)\n",
    "    \n",
    "# 0:Stopwords總數/ 1:含有stopwords的推文總數/ 2:（時間內的）推文總數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total4 = [0, 0, 0]\n",
    "for key in ordinarys:\n",
    "    _w, _s, _sc = getWordsDict(ordinarys[key].getText())\n",
    "    total4[0] += sum(_s.values())\n",
    "    total4[1] += _sc\n",
    "    total4[2] += len(ordinarys[key].inRangeDf)\n",
    "\n",
    "# 0:Stopwords總數/ 1:含有stopwords的推文總數/ 2:（時間內的）推文總數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trace_patients = go.Bar(x=['Type1', 'Type2'],\n",
    "                  y=[round(total3[0]/total3[1],1), round(total3[1]/total3[2],1)],\n",
    "                  name='Patients',\n",
    "                  marker=dict(color='#ffcdd2'))\n",
    "\n",
    "trace_ordinarys = go.Bar(x=['Type1', 'Type2'],\n",
    "                y=[round(total4[0]/total4[1],1), round(total4[1]/total4[2],1)],\n",
    "                name='Ordinarys',\n",
    "                marker=dict(color='#A2D5F2'))\n",
    "data = [trace_patients, trace_ordinarys]\n",
    "\n",
    "layout = go.Layout(title='比較',\n",
    "                xaxis=dict(title='使用者類型'),\n",
    "                yaxis=dict(title='平均數量'))\n",
    "\n",
    "fig3 = go.Figure(data=data, layout=layout)\n",
    "py.image.save_as(fig3, filename='img/a-simple-plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_tweet(txt):\n",
    "    \n",
    "    try:\n",
    "        # url\n",
    "        txt = re.sub(r\"http\\S+\", \"\", txt)\n",
    "        txt = re.sub(r\"https\\S+\", \"\", txt)\n",
    "        txt = re.sub(r\"twitter.com/\\S+\", \"\", txt)\n",
    "        txt = re.sub(r\"pic.twitter.com\\S+\", \"\", txt)\n",
    "        # mention\n",
    "        txt = re.sub(r\"@\\S+\", \"\", txt)\n",
    "    except TypeError as e:\n",
    "        print(e)\n",
    "    \n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_texts = []\n",
    "group_texts = []\n",
    "\n",
    "for key in ordinarys.keys():\n",
    "    base_texts.append(filter_tweet('\\n'.join(ordinarys[key].getText())))\n",
    "\n",
    "for key in patients.keys():\n",
    "    group_texts.append(filter_tweet('\\n'.join(patients[key].getText())))\n",
    "\n",
    "corpus = base_texts + group_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=128, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "idf = vectorizer.idf_\n",
    "Y = np.array([0] * len(base_texts) + [1]*len(group_texts), dtype=int)\n",
    "classifier  = RandomForestClassifier(n_jobs=-1, max_features=\"sqrt\", n_estimators=128)\n",
    "classifier.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_indicies = np.argsort(classifier.feature_importances_)\n",
    "vector2word = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = []\n",
    "for i in range(50):\n",
    "    vector = feature_indicies[-i-1]\n",
    "    word = vector2word[vector]\n",
    "    top_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['depression',\n",
       " 'pic',\n",
       " 'tus',\n",
       " 'wtf',\n",
       " 'status',\n",
       " 'diagnosed',\n",
       " 'wrong',\n",
       " 'com',\n",
       " 'bitch',\n",
       " 'pick',\n",
       " 'heart',\n",
       " 'care',\n",
       " 'piece',\n",
       " 'tatus',\n",
       " 'cry',\n",
       " 'falling',\n",
       " 'anxiety',\n",
       " 'okay',\n",
       " 'sorry',\n",
       " 'into',\n",
       " 'their',\n",
       " 'goal',\n",
       " 'never',\n",
       " 'read',\n",
       " 'cried',\n",
       " 'myself',\n",
       " 'happened',\n",
       " 'they',\n",
       " 'am',\n",
       " 'atus',\n",
       " 'person',\n",
       " 'finally',\n",
       " 'idk',\n",
       " 'old',\n",
       " 'and',\n",
       " 'tho',\n",
       " 'love',\n",
       " 'watched',\n",
       " 'said',\n",
       " 'alone',\n",
       " 'friends',\n",
       " 'felt',\n",
       " 'deserve',\n",
       " 'please',\n",
       " 'you',\n",
       " 'close',\n",
       " 'kind',\n",
       " 'la',\n",
       " 'easier',\n",
       " 'favorite']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "group_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
