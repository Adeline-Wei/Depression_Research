{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ipynb.fs.defs.PeopleInfo as peopleInfo\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import TweetTokenizer\n",
    "# from nltk.stem.porter import PorterStemmer\n",
    "# 引用 tweetTextHandler.ipynb\n",
    "import ipynb.fs.defs.TweetTextHandler as tweetTextHandler\n",
    "import ipynb.fs.defs.FilterMethods as filterMethods\n",
    "import sys, os\n",
    "sys.path.append('../2_feature')\n",
    "import ipynb.fs.defs.GetFeatures as getFeatures\n",
    "global stop_words\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getWordsDict(texts):\n",
    "    global stop_words\n",
    "    wordDict = dict()\n",
    "    stopwordDict = dict()\n",
    "    stopwordTweetCount = 0\n",
    "    tknzr = TweetTokenizer()\n",
    "\n",
    "    for txt in texts:\n",
    "        stopwordFlag = False\n",
    "        try:\n",
    "            txt = tweetTextHandler.del_url(txt)\n",
    "            for token in tknzr.tokenize(txt):\n",
    "                #token = porter.stem(token.lower())\n",
    "                if token.isdigit():\n",
    "                    pass\n",
    "                elif token in stop_words:\n",
    "                    stopwordDict[token] = stopwordDict.get(token, 0) + 1\n",
    "                    stopwordFlag = True\n",
    "                else:\n",
    "                    wordDict[token] = wordDict.get(token, 0) + 1\n",
    "            if stopwordFlag:\n",
    "                stopwordTweetCount += 1\n",
    "        except Exception as e:\n",
    "            #print(\"Error message:\", e)\n",
    "            pass\n",
    "    return wordDict, stopwordDict, stopwordTweetCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patients = dict()\n",
    "ordinarys = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('../0_dataset/patient_ids') as r:\n",
    "    for patient in r.readlines()[:100]:\n",
    "        patient = patient.strip()\n",
    "        patients[patient] = peopleInfo.Patient(patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove users:['181095972', '2409775369', '363595479', '2382824556', '113666708', '2195175613', '402683346', '3031516391', '1229103648']\n"
     ]
    }
   ],
   "source": [
    "patients = filterMethods.by_tweets_number(patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('../0_dataset/ordinary_ids') as r:\n",
    "    for ordinary in r.readlines()[:100]:\n",
    "        ordinary = ordinary.strip()\n",
    "        ordinarys[ordinary] = peopleInfo.Ordinary(ordinary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove users:[]\n"
     ]
    }
   ],
   "source": [
    "ordinarys = filterMethods.by_tweets_number(ordinarys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import plotly\n",
    "# import plotly.offline as offline\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "# from IPython.display import display\n",
    "plotly.tools.set_credentials_file(username='Adeline', api_key='Z5eltNtBQXqvI05ZFQtz')\n",
    "# offline.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total3 = [0, 0, 0]\n",
    "for key in patients:\n",
    "    _w, _s, _sc = getWordsDict(patients[key].getText())\n",
    "    total3[0] += sum(_s.values())\n",
    "    total3[1] += _sc\n",
    "    total3[2] += len(patients[key].inRangeDf)\n",
    "    \n",
    "# 0:Stopwords總數/ 1:含有stopwords的推文總數/ 2:（時間內的）推文總數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total4 = [0, 0, 0]\n",
    "for key in ordinarys:\n",
    "    _w, _s, _sc = getWordsDict(ordinarys[key].getText())\n",
    "    total4[0] += sum(_s.values())\n",
    "    total4[1] += _sc\n",
    "    total4[2] += len(ordinarys[key].inRangeDf)\n",
    "\n",
    "# 0:Stopwords總數/ 1:含有stopwords的推文總數/ 2:（時間內的）推文總數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trace_patients = go.Bar(x=['Type1', 'Type2'],\n",
    "                  y=[round(total3[0]/total3[1],1), round(total3[1]/total3[2],1)],\n",
    "                  name='Patients',\n",
    "                  marker=dict(color='#ffcdd2'))\n",
    "\n",
    "trace_ordinarys = go.Bar(x=['Type1', 'Type2'],\n",
    "                y=[round(total4[0]/total4[1],1), round(total4[1]/total4[2],1)],\n",
    "                name='Ordinarys',\n",
    "                marker=dict(color='#A2D5F2'))\n",
    "data = [trace_patients, trace_ordinarys]\n",
    "\n",
    "layout = go.Layout(title='比較',\n",
    "                xaxis=dict(title='使用者類型'),\n",
    "                yaxis=dict(title='平均數量'))\n",
    "\n",
    "fig3 = go.Figure(data=data, layout=layout)\n",
    "py.image.save_as(fig3, filename='img/a-simple-plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_texts = []\n",
    "group_texts = []\n",
    "\n",
    "for key in ordinarys.keys():\n",
    "    base_texts.extend(['\\n'.join(ordinarys[key].getText())])\n",
    "        \n",
    "for key in patients.keys():\n",
    "    group_texts.extend(['\\n'.join(patients[key].getText())])\n",
    "\n",
    "    corpus = base_texts + group_texts\n",
    "    vectorizer = TfidfVectorizer(min_df=1)\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    idf = vectorizer.idf_\n",
    "#     print (dict(zip(vectorizer.get_feature_names(), idf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98911"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
